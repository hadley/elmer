% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/api-ollama.R
\name{chat_ollama}
\alias{chat_ollama}
\title{Connect to a local ollama instances}
\usage{
chat_ollama(
  system_prompt = NULL,
  turns = NULL,
  base_url = "http://localhost:11434/v1",
  model,
  seed = NULL,
  api_args = list(),
  echo = FALSE
)
}
\arguments{
\item{system_prompt}{A system prompt to set the behavior of the assistant.}

\item{turns}{A list of turns to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch. Do not provide non-\code{NULL} values for both \code{turns} and
\code{system_prompt}.

Each message in the list should be a named list with at least \code{role}
(usually \code{system}, \code{user}, or \code{assistant}, but \code{tool} is also possible).
Normally there is also a \code{content} field, which is a string.}

\item{base_url}{The base URL to the endpoint; the default uses OpenAI.}

\item{model}{The model to use for the chat. The default, \code{NULL}, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.}

\item{seed}{Optional integer seed that ChatGPT uses to try and make output
more reproducible.}

\item{api_args}{Named list of arbitrary extra arguments passed to every
chat API call.}

\item{echo}{If \code{TRUE}, the \code{chat()} method streams the response to stdout by
default. (Note that this has no effect on the \code{stream()}, \code{chat_async()},
and \code{stream_async()} methods.)}
}
\description{
Download and install \href{https://ollama.com}{ollama} and then you can
chat with it from R with \code{chat_ollama()}. To install additional
models, use the \code{ollama} command line, e.g. \verb{ollama pull llama3.1}
or \verb{ollama pull gemma2}.

This function is a lightweight wrapper around \code{\link[=chat_openai]{chat_openai()}} with
the defaults tweaked for ollama.
}
