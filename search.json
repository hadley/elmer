[{"path":"https://hadley.github.io/elmer/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 elmer authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://hadley.github.io/elmer/articles/streaming-async.html","id":"streaming-results","dir":"Articles","previous_headings":"","what":"Streaming results","title":"Streaming and async APIs","text":"chat() method return results entire response received. (can print streaming results console, returns result response complete.) want process response arrives, can use stream() method. may useful want display response realtime, somewhere R console (like writing file, HTTP response, Shiny chat window); want manipulate response displaying , without giving immediacy streaming. stream() method returns generator coro package, can loop process response arrives.","code":"stream <- chat$stream(\"What are some common uses of R?\") coro::loop(for (chunk in stream) {   cat(toupper(chunk)) }) #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING COMPLEX STATISTICAL TESTS AND ANALYSES. #>  2. **DATA VISUALIZATION**: CREATING GRAPHS, CHARTS, AND PLOTS USING PACKAGES LIKE  GGPLOT2. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR AND TIDYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS WITH LIBRARIES LIKE CARET AND #>  RANDOMFOREST. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA AND GENOMIC STUDIES. #>  6. **ECONOMETRICS**: PERFORMING ECONOMIC DATA ANALYSIS AND MODELING. #>  7. **REPORTING**: GENERATING DYNAMIC REPORTS AND DASHBOARDS WITH R MARKDOWN. #>  8. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA AND FORECASTING. #> #>  THESE USES MAKE R A POWERFUL TOOL FOR DATA SCIENTISTS, STATISTICIANS, AND RESEARCHERS."},{"path":"https://hadley.github.io/elmer/articles/streaming-async.html","id":"async-usage","dir":"Articles","previous_headings":"","what":"Async usage","title":"Streaming and async APIs","text":"elmer also supports async usage, useful want run multiple chat sessions concurrently. primarily useful Shiny applications, using methods described block Shiny app users duration response. use async chat, instead chat()/stream(), call chat_async()/stream_async(). _async variants take arguments construction, return promises instead actual response. Remember chat objects stateful, maintaining conversation history interact . Note means doesn’t make sense issue multiple chat/stream operations chat object concurrently, conversation history become corrupted interleaved conversation fragments. need run multiple chat sessions concurrently, create multiple chat objects.","code":""},{"path":"https://hadley.github.io/elmer/articles/streaming-async.html","id":"asynchronous-chat","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous chat","title":"Streaming and async APIs","text":"asynchronous, non-streaming chat, use chat() method , handle result promise instead string. TODO: Shiny example","code":"library(promises)  chat$chat_async(\"How's your day going?\") %...>% print() #> I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions you have."},{"path":"https://hadley.github.io/elmer/articles/streaming-async.html","id":"asynchronous-streaming","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous streaming","title":"Streaming and async APIs","text":"asynchronous streaming, use stream() method , result async generator coro package. regular generator, except instead giving strings, gives promises resolve strings. Async generators advanced, require good understanding asynchronous programming R. also way present streaming results Shiny without blocking users. Fortunately, Shiny soon chat components make easier, can simply hand result stream_async() chat output.","code":"stream <- chat$stream_async(\"What are some common uses of R?\") coro::async(function() {   for (chunk in await_each(stream)) {     cat(toupper(chunk))   } })() #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING VARIOUS STATISTICAL TESTS AND MODELS. #>  2. **DATA VISUALIZATION**: CREATING PLOTS AND GRAPHS TO VISUALIZE DATA. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS AND ALGORITHMS. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA, ESPECIALLY IN GENOMICS. #>  6. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA FOR TRENDS AND FORECASTS. #>  7. **REPORT GENERATION**: CREATING DYNAMIC REPORTS WITH R MARKDOWN. #>  8. **GEOSPATIAL ANALYSIS**: MAPPING AND ANALYZING GEOGRAPHIC DATA."},{"path":"https://hadley.github.io/elmer/articles/tool-calling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tool calling (a.k.a. function calling)","text":"One interesting aspects modern chat models ability make use external tools defined caller. making chat request chat model, caller advertises one tools (defined function name, description, list expected arguments), chat model can choose respond one “tool calls”. tool calls requests chat model caller execute function given arguments; caller expected execute functions “return” results submitting another chat request conversation far, plus results. chat model can use results formulating response, , may decide make additional tool calls. Note chat model directly execute external tools! makes requests caller execute . value chat model brings helping execution, knowing makes sense call tool, values pass arguments, use results formulating response.","code":"library(elmer)"},{"path":"https://hadley.github.io/elmer/articles/tool-calling.html","id":"motivating-example","dir":"Articles","previous_headings":"Introduction","what":"Motivating example","title":"Tool calling (a.k.a. function calling)","text":"Let’s take look example really need external tool. Chat models generally know current time, makes questions like impossible. Unfortunately, example run September 18, 2024. Let’s give chat model ability determine current time try .","code":"chat <- chat_openai(model = \"gpt-4o\") chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") #> Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. To determine how long ago that #> was from the current year of 2023, we can calculate the difference in years, months, and days. #> #> From July 20, 1969, to July 20, 2023, is exactly 54 years. If today's date is after July 20, 2023, you #> would add the additional time since then. If it is before, you would consider slightly less than 54 #> years. #> #> As of right now, can you confirm the current date so we can calculate the precise duration?"},{"path":"https://hadley.github.io/elmer/articles/tool-calling.html","id":"defining-a-tool-function","dir":"Articles","previous_headings":"Introduction","what":"Defining a tool function","title":"Tool calling (a.k.a. function calling)","text":"first thing ’ll define R function returns current time. tool. Note ’ve gone trouble creating roxygen2 comments. important step help model use tool correctly! Let’s test :","code":"#' Gets the current time in the given time zone. #' #' @param tz The time zone to get the current time in. #' @return The current time in the given time zone. get_current_time <- function(tz = \"UTC\") {   format(Sys.time(), tz = tz, usetz = TRUE) } get_current_time() #> [1] \"2024-09-18 17:47:14 UTC\""},{"path":"https://hadley.github.io/elmer/articles/tool-calling.html","id":"registering-tools","dir":"Articles","previous_headings":"Introduction","what":"Registering tools","title":"Tool calling (a.k.a. function calling)","text":"Now need tell chat object get_current_time function. creating tool definition registering : fair amount code write, even simple function get_current_time. Fortunately, don’t write hand! generated register_tool call calling create_tool_metadata(get_current_time), printed code console. create_tool_metadata() works passing function’s signature documentation GPT-4o, asking generate register_tool call . Note create_tool_metadata() may create perfect results, must review generated code using . huge time-saver nonetheless, removes tedious boilerplate generation ’d otherwise.","code":"chat <- chat_openai(model = \"gpt-4o\")  chat$register_tool(ToolDef(   fun = get_current_time,   description = \"Gets the current time in the given time zone.\",   arguments = list(     tz = ToolArg(       type = \"string\",       description = \"The time zone to get the current time in. Defaults to `\\\"UTC\\\"`.\",       required = FALSE     )   ) ))"},{"path":"https://hadley.github.io/elmer/articles/tool-calling.html","id":"using-the-tool","dir":"Articles","previous_headings":"Introduction","what":"Using the tool","title":"Tool calling (a.k.a. function calling)","text":"’s need ! Let’s retry query: ’s correct! Without guidance, chat model decided call tool function successfully used result formulating response. (Full disclosure: originally tried example default model gpt-4o-mini got tool calling right date math wrong, hence explicit model=\"gpt-4o\".) tool example extremely simple, can imagine much interesting things tool functions: calling APIs, reading writing database, kicking complex simulation, even calling complementary GenAI model (like image generator). using elmer Shiny app, use tools set reactive values, setting chain reactive updates.","code":"chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") #> Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. #> #> To calculate the time elapsed from that moment until the current time (September 18, 2024, 17:47:19 #> UTC), we need to break it down. #> #> 1. From July 20, 1969, 20:17 UTC to July 20, 2024, 20:17 UTC is exactly 55 years. #> 2. From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, we need to further break down: #> #>    - From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, which is: #>      - 1 full month (August) #>      - 30 – 20 = 10 days of July #>      - 18 days of September until 17:47:19 UTC #> #> So, in detail: #>    - 55 years #>    - 1 month #>    - 28 days #>    - From July 20, 2024, 20:17 UTC to July 20, 2024, 17:47:19 UTC: 23 hours, 30 minutes, and 19 seconds #> #> Time Total: #> - 55 years #> - 1 month #> - 28 days #> - 23 hours #> - 30 minutes #> - 19 seconds #> #> This is the exact time that has elapsed since Neil Armstrong's historic touchdown on the moon."},{"path":"https://hadley.github.io/elmer/articles/tool-calling.html","id":"tool-limitations","dir":"Articles","previous_headings":"Introduction","what":"Tool limitations","title":"Tool calling (a.k.a. function calling)","text":"Remember tool arguments come chat model, tool results returned chat model. means simple, {jsonlite} compatible data types can used inputs outputs. ’s highly recommended stick strings/character, numbers, booleans/logical, null, named unnamed lists types. can forget using functions, environments, external pointers, R6 classes, complex R objects arguments return values. Returning data frames seems work OK, although careful return much data, counts tokens (.e., count context window limit also cost money).","code":""},{"path":"https://hadley.github.io/elmer/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hadley Wickham. Author, maintainer. Posit Software, PBC. Copyright holder, funder.","code":""},{"path":"https://hadley.github.io/elmer/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wickham H (2024). elmer: Call LLM APIs R. R package version 0.0.0.9000, https://hadley.github.io/elmer/, https://github.com/hadley/elmer.","code":"@Manual{,   title = {elmer: Call LLM APIs from R},   author = {Hadley Wickham},   year = {2024},   note = {R package version 0.0.0.9000, https://hadley.github.io/elmer/},   url = {https://github.com/hadley/elmer}, }"},{"path":"https://hadley.github.io/elmer/index.html","id":"elmer-","dir":"","previous_headings":"","what":"Call LLM APIs from R","title":"Call LLM APIs from R","text":"goal elmer provide user friendly wrapper common llm providers. Major design goals include support streaming making easy register call R functions.","code":""},{"path":"https://hadley.github.io/elmer/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Call LLM APIs from R","text":"can install development version elmer GitHub :","code":"# install.packages(\"pak\") pak::pak(\"hadley/elmer\")"},{"path":"https://hadley.github.io/elmer/index.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"Call LLM APIs from R","text":"Depending backend use, ’ll need set appropriate environment variable ~/.Renviron (easy way open file call usethis::edit_r_environ()): chat_claude(), set ANTHROPIC_API_KEY using key https://console.anthropic.com/account/keys. chat_gemini(), set GOOGLE_API_KEY using key https://aistudio.google.com/app/apikey. chat_openai() set OPENAI_API_KEY using key https://platform.openai.com/account/api-keys.","code":""},{"path":"https://hadley.github.io/elmer/index.html","id":"using-elmer","dir":"","previous_headings":"","what":"Using elmer","title":"Call LLM APIs from R","text":"chat elmer several different ways, depending whether working interactively programmatically. start creating new chat object: Chat objects stateful: retain context conversation, new query can build previous ones. true regardless various ways chatting use.","code":"library(elmer)  chat <- chat_openai(   model = \"gpt-4o-mini\",   system_prompt = \"You are a friendly but terse assistant.\",   echo = TRUE )"},{"path":"https://hadley.github.io/elmer/index.html","id":"interactive-chat-console","dir":"","previous_headings":"Using elmer","what":"Interactive chat console","title":"Call LLM APIs from R","text":"interactive, least programmatic way using elmer chat directly R console live_console(chat) browser live_browser(). chat console useful quickly exploring capabilities model, especially ’ve customized chat object tool integrations (see ). , keep mind chat object retains state, enter chat console, previous interactions chat object still part conversation, interactions chat console persist even exit back R prompt.","code":"live_console(chat) #> ╔════════════════════════════════════════════════════════╗ #> ║  Entering chat console. Use \"\"\" for multi-line input.  ║ #> ║  Press Ctrl+C to quit.                                 ║ #> ╚════════════════════════════════════════════════════════╝ #> >>> Who were the original creators of R? #> R was originally created by Ross Ihaka and Robert Gentleman at the University of #> Auckland, New Zealand. #> #> >>> When was that? #> R was initially released in 1995. Development began a few years prior to that, #> in the early 1990s."},{"path":"https://hadley.github.io/elmer/index.html","id":"interactive-method-call","dir":"","previous_headings":"Using elmer","what":"Interactive method call","title":"Call LLM APIs from R","text":"second interactive way chat using elmer call chat() method. initialize chat object echo = TRUE, , chat method streams response console arrives. entire response received, returned character vector (invisibly, ’s printed twice). mode useful want see response arrives, don’t want enter chat console.","code":"chat$chat(\"What preceding languages most influenced R?\") #> R was primarily influenced by the S programming language, particularly S-PLUS. #> Other languages that had an impact include Scheme and various data analysis #> languages."},{"path":"https://hadley.github.io/elmer/index.html","id":"vision-image-input","dir":"","previous_headings":"Using elmer > Interactive method call","what":"Vision (image input)","title":"Call LLM APIs from R","text":"want ask question image, can pass one additional input arguments using content_image_file() /content_image_url(). content_image_url function takes URL image file sends URL directly API. content_image_file function takes path local image file encodes base64 string send API. Note default, content_image_file automatically resizes image fit within 512x512 pixels; set resize parameter \"high\" higher resolution needed.","code":"chat$chat(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   \"Can you explain this logo?\" ) #> The logo of R features a stylized letter \"R\" in blue, enclosed in an oval shape that resembles the letter \"O,\" #> signifying the programming language's name. The design conveys a modern and professional look, reflecting its use #> in statistical computing and data analysis. The blue color often represents trust and reliability, which aligns #> with R's role in data science."},{"path":"https://hadley.github.io/elmer/index.html","id":"programmatic-chat","dir":"","previous_headings":"Using elmer","what":"Programmatic chat","title":"Call LLM APIs from R","text":"don’t want see response arrives, can turn echoing leaving echo = TRUE argument chat_openai(). mode useful programming using elmer, result either intended human consumption want process response displaying .","code":"chat <- chat_openai(   model = \"gpt-4o-mini\",   system_prompt = \"You are a friendly but terse assistant.\" ) chat$chat(\"Is R a functional programming language?\") #> [1] \"Yes, R supports functional programming concepts. It allows functions to be first-class objects, supports higher-order functions, and encourages the use of functions as core components of code. However, it also supports procedural and object-oriented programming styles.\""},{"path":"https://hadley.github.io/elmer/index.html","id":"learning-more","dir":"","previous_headings":"","what":"Learning more","title":"Call LLM APIs from R","text":"Learn streaming async APIs vignette(\"streaming-async\"). Learn tool calling (aka function calling) vignette(\"tool-calling\").","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":null,"dir":"Reference","previous_headings":"","what":"A chat — Chat","title":"A chat — Chat","text":"Chat sequence sequence user assistant Turns sent specific Provider. Chat mutable R6 object takes care managing state associated chat; .e. records messages send server, messages receive back. register tool (.e. R function assistant can call behalf), also takes care tool loop. generally create object , instead call chat_openai() friends instead.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A chat — Chat","text":"promise resolves string (probably Markdown).","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"A chat — Chat","text":"system_prompt system prompt, present, string. Otherwise, NULL.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"A chat — Chat","text":"Chat$new() Chat$turns() Chat$tokens() Chat$last_turn() Chat$chat() Chat$chat_async() Chat$stream() Chat$stream_async() Chat$register_tool() Chat$clone()","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$new(provider, turns, seed = NULL, echo = \"none\")"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"provider provider object. turns unnamed list turns start chat (.e., continuing previous conversation). NULL zero-length list, conversation begins scratch. seed Optional integer seed ChatGPT uses try make output reproducible. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-turns-","dir":"Reference","previous_headings":"","what":"Method turns()","title":"A chat — Chat","text":"turns sent received far (optionally starting system prompt, ).","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$turns(include_system_prompt = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"include_system_prompt Whether include system prompt turns (exists).","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-tokens-","dir":"Reference","previous_headings":"","what":"Method tokens()","title":"A chat — Chat","text":"List number tokens consumed assistant turn. Currently tokens recorded assistant turns ; user turns zeros.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$tokens()"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-last-turn-","dir":"Reference","previous_headings":"","what":"Method last_turn()","title":"A chat — Chat","text":"last turn returned assistant.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$last_turn(role = c(\"assistant\", \"user\", \"system\"))"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"role Optionally, specify role find last turn role.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"A chat — Chat","text":"Either Turn NULL, turns specified role occurred.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"A chat — Chat","text":"Submit input chatbot, return response simple string (probably Markdown).","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat(..., echo = NULL)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images (see content_image_file() content_image_url(). echo Whether emit response stdout received. NULL, value echo set chat object created used.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-chat-async-","dir":"Reference","previous_headings":"","what":"Method chat_async()","title":"A chat — Chat","text":"Submit input chatbot, receive promise resolves response .","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat_async(...)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-stream-","dir":"Reference","previous_headings":"","what":"Method stream()","title":"A chat — Chat","text":"Submit input chatbot, returning streaming results. Returns coro generator yields strings. iterating, generator block waiting content chatbot.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$stream(...)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-stream-async-","dir":"Reference","previous_headings":"","what":"Method stream_async()","title":"A chat — Chat","text":"Submit input chatbot, returning asynchronously streaming results. Returns coro async generator yields string promises.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$stream_async(...)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-register-tool-","dir":"Reference","previous_headings":"","what":"Method register_tool()","title":"A chat — Chat","text":"Register tool (R function) chatbot can use. chatbot decides use function,  elmer automatically call submit results back.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$register_tool(tool_def)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"tool_def Tool definition created ToolDef.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"A chat — Chat","text":"objects class cloneable method.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$clone(deep = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"deep Whether make deep clone.","code":""},{"path":"https://hadley.github.io/elmer/reference/Content.html","id":null,"dir":"Reference","previous_headings":"","what":"Content types received from and sent to a chatbot — Content","title":"Content types received from and sent to a chatbot — Content","text":"elmer abstracts away differences way different Providers represent various types content, allowing easily write code works chatbot. set classes represents various types content can sent received provider: ContentText: simple text (often markdown format). type content can streamed live received. ContentImageRemote ContentImageInline: images, either pointer remote URL included inline object. See content_image_file() friends convenient ways construct objects. ContentToolRequest: request perform tool call (sent assistant). ContentToolResult: result calling tool (sent user).","code":""},{"path":"https://hadley.github.io/elmer/reference/Content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content()  ContentText(text = class_missing)  ContentImage()  ContentImageRemote(url = class_missing, detail = class_missing)  ContentImageInline(type = class_missing, data = class_missing)  ContentToolRequest(   id = class_missing,   name = class_missing,   arguments = class_missing )  ContentToolResult(   id = class_missing,   value = class_missing,   error = class_missing )"},{"path":"https://hadley.github.io/elmer/reference/Content.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Content types received from and sent to a chatbot — Content","text":"text single string. url URL remote image. detail currently used. type MIME type image. data Base64 encoded image data. id Tool call id (used associate request result) name Function name arguments Named list arguments call function . value, error Either results calling function succeeded, otherwise error message, string. One value error always NULL.","code":""},{"path":"https://hadley.github.io/elmer/reference/Provider.html","id":null,"dir":"Reference","previous_headings":"","what":"A chatbot provider — Provider","title":"A chatbot provider — Provider","text":"Provider captures details one chatbot service/API. captures API works, details underlying large language model. Different providers might offer (open source) model behind different API.","code":""},{"path":"https://hadley.github.io/elmer/reference/Provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chatbot provider — Provider","text":"","code":"Provider(base_url = class_missing, extra_args = class_missing)"},{"path":"https://hadley.github.io/elmer/reference/Provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chatbot provider — Provider","text":"base_url base URL API. extra_args Arbitrary extra arguments included request body.","code":""},{"path":"https://hadley.github.io/elmer/reference/Provider.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A chatbot provider — Provider","text":"add support new backend, need subclass Provider (adding additional fields provider needs) implement various generics control behavior provider.","code":""},{"path":"https://hadley.github.io/elmer/reference/ToolArg.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a tool — ToolDef","title":"Define a tool — ToolDef","text":"Define R function use chatbot. function always run current R instance. Learn vignette(\"tool-calling\").","code":""},{"path":"https://hadley.github.io/elmer/reference/ToolArg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a tool — ToolDef","text":"","code":"ToolDef(fun, name, description, arguments = list(), ...)  ToolArg(type, description, required = TRUE, ...)"},{"path":"https://hadley.github.io/elmer/reference/ToolArg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a tool — ToolDef","text":"fun function invoked tool called. name name function. description Description argument free text. arguments named list arguments function accepts. named list objects created ToolArg(). ... Additional provider specific JSON Schema properties (e.g. properties, enum, pattern). example, OpenAI supports strict parameter TRUE enables Structured Output mode, comes number additional requirements. type Argument type (\"null\", \"boolean\", \"object\", \"array\", \"number\", \"string\"). required argument required?","code":""},{"path":"https://hadley.github.io/elmer/reference/ToolArg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a tool — ToolDef","text":"","code":"if (FALSE) { # elmer:::openai_key_exists()  # First define the metadata that the model uses to figure out when to # call the tool tool_rnorm <- ToolDef(   rnorm,   description = \"Drawn numbers from a random normal distribution\",   arguments = list(     n = ToolArg(       type = \"integer\",       description = \"The number of observations. Must be a positive integer.\"     ),     mean = ToolArg(       type = \"number\",       description = \"The mean value of the distribution.\"     ),     sd = ToolArg(       type = \"number\",       description = \"The standard deviation of the distribution. Must be a non-negative number.\"     )   ) ) chat <- chat_openai() # Then register it chat$register_tool(tool_rnorm)  # Then ask a question that needs it. chat$chat(\"   Give me five numbers from a random normal distribution. \")  # Look at the chat history to see how tool calling works: # Assistant sends a tool request which is evaluated locally and # results are send back in a tool result. chat }"},{"path":"https://hadley.github.io/elmer/reference/Turn.html","id":null,"dir":"Reference","previous_headings":"","what":"A user or assistant turn — Turn","title":"A user or assistant turn — Turn","text":"Every conversation chatbot consists pairs user assistant turns, corresponding HTTP request response. turns represented Turn object, contains list Contents representing individual messages within turn. might text, images, tool requests (assistant ), tool responses (user ). Note call $chat() related functions may result multiple user-assistant turn cycles. example, registered tools, elmer automatically handle tool calling loop, may result number additional cycles. Learn tool calling vignette(\"tool-calling\").","code":""},{"path":"https://hadley.github.io/elmer/reference/Turn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A user or assistant turn — Turn","text":"","code":"Turn(role, contents = list(), json = list(), tokens = c(0, 0))"},{"path":"https://hadley.github.io/elmer/reference/Turn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A user or assistant turn — Turn","text":"role Either \"user\", \"assistant\", \"system\". contents list Content objects. json serialized JSON corresponding underlying data turns. Currently provided assistant. useful information returned provider elmer otherwise expose. tokens numeric vector length 2 representing number input output tokens (respectively) used turn. Currently recorded assistant turns.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_azure.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Azure OpenAI — chat_azure","title":"Chat with a model hosted on Azure OpenAI — chat_azure","text":"Azure OpenAI server hosts number open source models well proprietary models OpenAI.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_azure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Azure OpenAI — chat_azure","text":"","code":"chat_azure(   endpoint = azure_endpoint(),   deployment_id,   api_version = NULL,   system_prompt = NULL,   turns = NULL,   api_key = azure_key(),   token = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://hadley.github.io/elmer/reference/chat_azure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Azure OpenAI — chat_azure","text":"endpoint Azure OpenAI endpoint url protocol hostname, .e. https://{-resource-name}.openai.azure.com. Defaults using value AZURE_OPENAI_ENDPOINT envinronment variable. deployment_id Deployment id model want use. api_version API version use. system_prompt system prompt set behavior assistant. turns list turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values turns system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. api_key API key use authentication. generally supply directly, instead set AZURE_OPENAI_API_KEY environment variable. token Azure token authentication. typically required Azure OpenAI API calls, can used setup requires . api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_azure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Azure OpenAI — chat_azure","text":"Chat object.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_claude.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an Anthropic Claude model — chat_claude","title":"Chat with an Anthropic Claude model — chat_claude","text":"Anthropic provides number chat based models Claude moniker. Note Claude Prop membership give ability call models via API. need go developer console sign (pay ) developer account give API key can use package.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_claude.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an Anthropic Claude model — chat_claude","text":"","code":"chat_claude(   system_prompt = NULL,   turns = NULL,   max_tokens = 4096,   model = NULL,   api_args = list(),   base_url = \"https://api.anthropic.com/v1\",   api_key = anthropic_key(),   echo = NULL )"},{"path":"https://hadley.github.io/elmer/reference/chat_claude.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an Anthropic Claude model — chat_claude","text":"system_prompt system prompt set behavior assistant. turns list turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values turns system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. max_tokens Maximum number tokens generate stopping. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. api_args Named list arbitrary extra arguments appended body every chat API call. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set ANTHROPIC_API_KEY environment variable. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_claude.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an Anthropic Claude model — chat_claude","text":"Chat object.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/chat_cortex.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"Chat LLM-powered Snowflake Cortex Analyst. Unlike comparable model APIs, Cortex take system prompt. Instead, caller must provide \"semantic model\" describing available tables, meaning, verified queries can run starting point. semantic model can passed YAML string via reference existing file Snowflake Stage. Note Cortex support multi-turn, remember previous messages. support registering tools, attempting result error.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_cortex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"","code":"chat_cortex(   account = Sys.getenv(\"SNOWFLAKE_ACCOUNT\"),   credentials = cortex_credentials,   model_spec = NULL,   model_file = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )  cortex_credentials(account = Sys.getenv(\"SNOWFLAKE_ACCOUNT\"))"},{"path":"https://hadley.github.io/elmer/reference/chat_cortex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"account Snowflake account identifier, e.g. \"testorg-test_account\". credentials list authentication headers pass httr2::req_headers() function returns passed account parameter. default cortex_credentials() function picks ambient Snowflake OAuth key-pair authentication credentials handles refreshing automatically. model_spec semantic model specification, NULL using model_file instead. model_file Path semantic model file stored Snowflake Stage, NULL using model_spec instead. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_cortex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"Chat object.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_cortex.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"cortex_credentials() picks following ambient Snowflake credentials: static OAuth token defined via SNOWFLAKE_TOKEN environment variable. Key-pair authentication credentials defined via SNOWFLAKE_USER SNOWFLAKE_PRIVATE_KEY (can PEM-encoded private key path one) environment variables. Posit Workbench-managed Snowflake credentials corresponding account.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/chat_cortex.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"","code":"if (FALSE) { # elmer:::cortex_credentials_exist() chat <- chat_cortex(   model_file = \"@my_db.my_schema.my_stage/model.yaml\" ) chat$chat(\"What questions can I ask?\") }"},{"path":"https://hadley.github.io/elmer/reference/chat_gemini.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a Google Gemini model — chat_gemini","title":"Chat with a Google Gemini model — chat_gemini","text":"Chat Google Gemini model","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_gemini.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a Google Gemini model — chat_gemini","text":"","code":"chat_gemini(   system_prompt = NULL,   turns = NULL,   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = gemini_key(),   model = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://hadley.github.io/elmer/reference/chat_gemini.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a Google Gemini model — chat_gemini","text":"system_prompt system prompt set behavior assistant. turns list turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values turns system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GOOGLE_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_gemini.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a Google Gemini model — chat_gemini","text":"Chat object.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/chat_github.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on the GitHub model marketplace — chat_github","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"GitHub (via Azure) hosts number open source OpenAI models. access GitHub model marketplace, need apply accepted beta access program. See https://github.com/marketplace/models details. function lightweight wrapper around chat_openai() defaults tweaked GitHub model marketplace.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_github.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"chat_github(   system_prompt = NULL,   turns = NULL,   base_url = \"https://models.inference.ai.azure.com/\",   api_key = github_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://hadley.github.io/elmer/reference/chat_github.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"system_prompt system prompt set behavior assistant. turns list turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values turns system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead manage GitHub credentials described https://usethis.r-lib.org/articles/git-credentials.html. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_github.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"Chat object.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/chat_groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Groq — chat_groq","title":"Chat with a model hosted on Groq — chat_groq","text":"Sign https://groq.com. function lightweight wrapper around chat_openai() defaults tweaked groq.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"chat_groq(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.groq.com/openai/v1\",   api_key = groq_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://hadley.github.io/elmer/reference/chat_groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Groq — chat_groq","text":"system_prompt system prompt set behavior assistant. turns list turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values turns system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/chat_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a local ollama model — chat_ollama","title":"Chat with a local ollama model — chat_ollama","text":"use chat_ollama() first download install ollama. install models command line, e.g. ollama pull llama3.1 ollama pull gemma2. function lightweight wrapper around chat_openai() defaults tweaked ollama.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a local ollama model — chat_ollama","text":"","code":"chat_ollama(   system_prompt = NULL,   turns = NULL,   base_url = \"http://localhost:11434/v1\",   model,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://hadley.github.io/elmer/reference/chat_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a local ollama model — chat_ollama","text":"system_prompt system prompt set behavior assistant. turns list turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values turns system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/chat_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an OpenAI model — chat_openai","title":"Chat with an OpenAI model — chat_openai","text":"OpenAI provides number chat based models ChatGPT moniker. Note ChatGPT Plus membership give ability call models via API. need go developer platform sign (pay ) developer account give API key can use package.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat_openai(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.openai.com/v1\",   api_key = openai_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://hadley.github.io/elmer/reference/chat_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an OpenAI model — chat_openai","text":"system_prompt system prompt set behavior assistant. turns list turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values turns system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an OpenAI model — chat_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/chat_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an OpenAI model — chat_openai","text":"","code":"if (FALSE) { # elmer:::openai_key_exists() chat <- chat_openai() chat$chat(\"   What is the difference between a tibble and a data frame?   Answer with a bulleted list \") }"},{"path":"https://hadley.github.io/elmer/reference/chat_perplexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on perplexity.ai — chat_perplexity","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Sign https://www.perplexity.ai. function lightweight wrapper around chat_openai() defaults tweaked groq.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_perplexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"chat_perplexity(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.perplexity.ai/\",   api_key = perplexity_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://hadley.github.io/elmer/reference/chat_perplexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"system_prompt system prompt set behavior assistant. turns list turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values turns system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set PERPLEXITY_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_perplexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Chat object.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode image content for chat input — content_image_url","title":"Encode image content for chat input — content_image_url","text":"functions used prepare image URLs files input chatbot. content_image_url() function used provide URL image, content_image_file() used provide image data .","code":""},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode image content for chat input — content_image_url","text":"","code":"content_image_url(url, detail = c(\"auto\", \"low\", \"high\"))  content_image_file(path, content_type = \"auto\", resize = \"low\")  content_image_plot(width = 768, height = 768)"},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode image content for chat input — content_image_url","text":"url URL image include chat input. Can data: URL regular URL. Valid image types PNG, JPEG, WebP, non-animated GIF. detail detail setting image. Can \"auto\", \"low\", \"high\". path path image file include chat input. Valid file extensions .png, .jpeg, .jpg, .webp, (non-animated) .gif. content_type content type image (e.g. image/png). \"auto\", content type inferred file extension. resize \"low\", resize images fit within 512x512. \"high\", resize fit within 2000x768 768x2000. (See OpenAI docs specific sizes used.) \"none\", resize. can also pass custom string resize image specific size, e.g. \"200x200\" resize 200x200 pixels preserving aspect ratio. Append > resize image larger specified size, ! ignore aspect ratio (e.g. \"300x200>!\"). values none require magick package. width, height Width height pixels.","code":""},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode image content for chat input — content_image_url","text":"input object suitable including ... parameter chat(), stream(), chat_async(), stream_async() methods.","code":""},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encode image content for chat input — content_image_url","text":"","code":"if (FALSE) { # elmer:::openai_key_exists() chat <- chat_openai(echo = TRUE) chat$chat(   \"What do you see in these images?\",   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   content_image_file(system.file(\"httr2.png\", package = \"elmer\")) )  DONTSHOW({dev.control('enable')}) plot(waiting ~ eruptions, data = faithful) chat <- chat_openai(echo = TRUE) chat$chat(   \"Describe this plot in one paragraph, as suitable for inclusion in    alt-text. You should briefly describe the plot type, the axes, and    2-5 major visual patterns.\",    content_image_plot() ) }"},{"path":"https://hadley.github.io/elmer/reference/create_tool_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Create metadata for a tool — create_tool_def","title":"Create metadata for a tool — create_tool_def","text":"order use function tool chat, need craft right ToolDef. function helps documented functions extracting function's R documentation creating ToolDef() call , using LLM. meant used interactively writing code, part final code. function package documentation, used. Otherwise, source code function can automatically detected, comments immediately preceding function used (especially helpful Roxygen comments). neither available, just function signature used. Note function inherently imperfect. handle possible R functions, parameters suitable use tool call (example, serializable simple JSON objects). documentation might specify expected shape arguments level detail allow exact JSON schema generated. Please sure review generated code using !","code":""},{"path":"https://hadley.github.io/elmer/reference/create_tool_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create metadata for a tool — create_tool_def","text":"","code":"create_tool_def(topic, model = \"gpt-4o\", echo = interactive(), verbose = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/create_tool_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create metadata for a tool — create_tool_def","text":"topic symbol string literal naming function create metadata . Can also expression form pkg::fun. model OpenAI model use generating metadata. Defaults \"gpt-4o\", highly recommended \"gpt-4o-mini\". echo Emit registration code console. Defaults TRUE interactive sessions. verbose TRUE, print input send LLM, may useful debugging unexpectedly poor results.","code":""},{"path":"https://hadley.github.io/elmer/reference/create_tool_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create metadata for a tool — create_tool_def","text":"register_tool call can copy paste code. Returned invisibly echo TRUE.","code":""},{"path":"https://hadley.github.io/elmer/reference/create_tool_def.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create metadata for a tool — create_tool_def","text":"","code":"if (FALSE) { # \\dontrun{   # These are all equivalent   create_tool_def(rnorm)   create_tool_def(stats::rnorm)   create_tool_def(\"rnorm\") } # }"},{"path":"https://hadley.github.io/elmer/reference/elmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"elmer: Call LLM APIs from R — elmer-package","title":"elmer: Call LLM APIs from R — elmer-package","text":"consistent interface calling LLM APIs. Includes support streaming.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/elmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"elmer: Call LLM APIs from R — elmer-package","text":"Maintainer: Hadley Wickham hadley@posit.co contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://hadley.github.io/elmer/reference/interpolate.html","id":null,"dir":"Reference","previous_headings":"","what":"Helpers for interpolating data into prompts — interpolate","title":"Helpers for interpolating data into prompts — interpolate","text":"functions lightweight wrappers around glue make easier interpolate dynamic data static prompt. Compared glue, functions expect wrap dynamic values {{ }}, making easier include R code JSON prompt.","code":""},{"path":"https://hadley.github.io/elmer/reference/interpolate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"interpolate(prompt, ..., .envir = parent.frame())  interpolate_file(path, ..., .envir = parent.frame())"},{"path":"https://hadley.github.io/elmer/reference/interpolate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helpers for interpolating data into prompts — interpolate","text":"prompt prompt string. generally expose end user, since glue interpolation makes easy run arbitrary code. ... Define additional temporary variables substitution. .envir Environment evaluate ... expressions . Used wrapping another function. See vignette(\"wrappers\", package = \"glue\") details. path path prompt file (often .md).","code":""},{"path":"https://hadley.github.io/elmer/reference/interpolate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"joke <- \"You're a cool dude who loves to make jokes. Tell me a joke about {{topic}}.\"  # You can supply valuese directly: interpolate(joke, topic = \"bananas\") #> You're a cool dude who loves to make jokes. Tell me a joke about bananas.  # Or allow interpolate to find them in the current environment: topic <- \"applies\" interpolate(joke) #> You're a cool dude who loves to make jokes. Tell me a joke about applies."},{"path":"https://hadley.github.io/elmer/reference/live_console.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a live chat application — live_console","title":"Open a live chat application — live_console","text":"live_console() lets chat interactively console. live_browser() lets chat interactively browser. Note functions mutate input chat object chat turns appended history.","code":""},{"path":"https://hadley.github.io/elmer/reference/live_console.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a live chat application — live_console","text":"","code":"live_console(chat, quiet = FALSE)  live_browser(chat, quiet = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/live_console.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a live chat application — live_console","text":"chat chat object created chat_openai() friends. quiet TRUE, suppresses initial message explains use console.","code":""},{"path":"https://hadley.github.io/elmer/reference/live_console.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open a live chat application — live_console","text":"(Invisibly) input chat.","code":""},{"path":"https://hadley.github.io/elmer/reference/token_usage.html","id":null,"dir":"Reference","previous_headings":"","what":"Report on token usage in the current session — token_usage","title":"Report on token usage in the current session — token_usage","text":"Call function find cumulative number tokens sent recieved current session.","code":""},{"path":"https://hadley.github.io/elmer/reference/token_usage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage()"},{"path":"https://hadley.github.io/elmer/news/index.html","id":"elmer-development-version","dir":"Changelog","previous_headings":"","what":"elmer (development version)","title":"elmer (development version)","text":"content_image_url() now create inline images given data url (#110). Streaming ollama results works (#117). Streaming OpenAI results now capture results, including logprops (#115). New interpolate() prompt_file() make easier create prompts mix static text dynamic values. can find many tokens ’ve used current session calling token_usage(). chat_browser() chat_console() now live_browser() live_console(). echo can now one three values: “none”, “text”, “”. “”, ’ll now see user assistant turns, content types printed, just text. running global environment, echo defaults “text”, running inside function defaults “none”. chat$register_tool() now takes object created ToolDef(). makes little easier reuse tool definitions (#32). new_chat_openai() now chat_openai(). Claude Gemini now supported via chat_claude() chat_gemini(). Snowflake Cortex Analyst now supported via chat_cortex() (#56).","code":""}]
