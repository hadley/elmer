[{"path":"https://ellmer.tidyverse.org/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 ellmer authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"vocabulary","dir":"Articles","previous_headings":"","what":"Vocabulary","title":"Getting started with ellmer","text":"’ll start laying key vocab ’ll need understand LLMs. Unfortunately vocab little entangled: understand one term ’ll often know little others. ’ll start simple definitions important terms iteratively go little deeper. starts prompt, text (typically question request) send LLM. starts conversation, sequence turns alternate user prompts model responses. Inside model, prompt response represented sequence tokens, represent either individual words subcomponents word. tokens used compute cost using model measure size context, combination current prompt previous prompts responses used generate next response. ’s useful make distinction providers models. provider web API gives access one models. distinction bit subtle providers often synonymous model, like OpenAI GPT, Anthropic Claude, Google Gemini. providers, like Ollama, can host many different models, typically open source models like LLaMa Mistral. Still providers support open closed models, typically partnering company provides popular closed model. example, Azure OpenAI offers open source models OpenAI’s GPT, AWS Bedrock offers open source models Anthropic’s Claude.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"what-is-a-token","dir":"Articles","previous_headings":"Vocabulary","what":"What is a token?","title":"Getting started with ellmer","text":"LLM model, like models needs way represent inputs numerically. LLMs, means need way convert words numbers. goal tokenizer. example, using GPT 4o tokenizer, string “R created?” converted 5 tokens: 5958 (“”), 673 (” ”), 460 (” R”), 5371 (” created”), 30 (“?”). can see, many simple strings can represented single token. complex strings require multiple tokens. example, string “counterrevolutionary” requires 4 tokens: 32128 (“counter”), 264 (“re”), 9477 (“volution”), 815 (“ary”). (can see various strings tokenized http://tiktokenizer.vercel.app/). ’s important rough sense text converted tokens tokens used determine cost model much context can used predict next response. average English word needs ~1.5 tokens page might require 375-400 tokens complete book might require 75,000 150,000 tokens. languages typically require tokens, (brief) LLMs trained data internet, primarily English. LLMs priced per million tokens. State art models (like GPT-4o Claude 3.5 sonnet) cost $2-3 per million input tokens, $10-15 per million output tokens. Cheaper models can cost much less, e.g. GPT-4o mini costs $0.15 per million input tokens $0.60 per million output tokens. Even $10 API credit give lot room experimentation, particularly cheaper models, prices likely decline model performance improves. Tokens also used measure context window, much text LLM can use generate next response. ’ll discuss shortly, context length includes full state conversation far (prompts model’s responses), means cost grow rapidly number conversational turns. ellmer, can see many tokens conversations used printing , can see total usage session token_usage(). want learn tokens tokenizers, ’d recommend watching first 20-30 minutes Let’s build GPT Tokenizer Andrej Karpathy. certainly don’t need learn build tokenizer, intro give bunch useful background knowledge help improve undersstanding LLM’s work.","code":"chat <- chat_openai(model = \"gpt-4o\") . <- chat$chat(\"Who created R?\", echo = FALSE) chat #> <Chat turns=2 tokens=11/44> #> ── user ────────────────────────────────────────────────────────────────── #> Who created R? #> ── assistant ───────────────────────────────────────────────────────────── #> R was created by statisticians Ross Ihaka and Robert Gentleman at the #> University of Auckland, New Zealand, in the mid-1990s. It was developed #> as a free software environment for statistical computing and graphics.  token_usage() #>                               name input output #> 1 OpenAI-https://api.openai.com/v1    11     44"},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"what-is-a-conversation","dir":"Articles","previous_headings":"Vocabulary","what":"What is a conversation?","title":"Getting started with ellmer","text":"conversation LLM takes place series HTTP requests responses: send question LLM HTTP request, sends back reply HTTP response. words, conversation consists sequence paired turns: sent prompt returned response. ’s important note request includes current user prompt, every previous user prompt model response. means : cost conversation grows quadratically number turns: want save money, keep conversations short. response affected previous prompts responses. can make converstion get stuck local optimum, ’s generally better iterate starting new conversation better prompt rather long back--forth. ellmer full control conversational history. ’s ellmer’s responsibility send previous turns conversation, ’s possible start conversation one model finish another.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"what-is-a-prompt","dir":"Articles","previous_headings":"Vocabulary","what":"What is a prompt?","title":"Getting started with ellmer","text":"user prompt question send model. two important prompts underlie user prompt: core system prompt, unchangeable, set model provider, affects every conversation. can see look like Anthropic, publishes core system prompts. system prompt, set create new conversation, affects every response. ’s used provide additional instructions model, shaping responses needs. example, might use system prompt ask model always respond Spanish write dependency-free base R code. can also use system prompt provide model information wouldn’t otherwise know, like details database schema, preferred ggplot2 theme color palette. use chat app like ChatGPT claude.ai can iterate user prompt. ’re programming LLMs, ’ll primarily iterate system prompt. example, ’re developing app helps user write tidyverse code, ’d work system prompt ensure user gets style code want. Writing good prompt, called prompt design, key effective use LLMs. discussed detail vignette(\"prompt-design\").","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"example-uses","dir":"Articles","previous_headings":"","what":"Example uses","title":"Getting started with ellmer","text":"Now ’ve got basic vocab belt, ’m going fire bunch interesting potential use cases . special purpose tools might solve cases faster /cheaper, LLM allows rapidly prototype solution. can extremely valuable even end using specialised tools final product. general, recommend avoiding LLMs accuracy critical. said, still many cases use. example, even though always requires manual fiddling, might save bunch time evern 80% correct solution. fact, even --good solution can still useful makes easier get started: ’s easier react something rather start scratch blank page.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"chatbots","dir":"Articles","previous_headings":"Example uses","what":"Chatbots","title":"Getting started with ellmer","text":"great place start ellmer LLMs build chatbot custom prompt. Chatbots familiar interface LLMs easy create R shinychat. ’s surprising amount value creating custom chatbot prompt stuffed useful knowledge. example: Help people use new package. , need custom prompt LLMs trained data prior package’s existence. can create surprisingly useful tool just preloading prompt README vignettes. ellmer assistant works. Build language specific prompts R /python. Shiny assistant helps build shiny apps (either R python) combining prompt gives general advice building apps prompt R python. python prompt detailed ’s much less information Shiny Python existing LLM knowledgebases. Help people find answers questions. Even ’ve written bunch documentation something, might find still get questions folks can’t easily find exactly ’re looking . can reduce need answer questions creating chatbot prompt contains documentation. example, ’re teacher, create chatbot includes syllabus prompt. eliminates common class question data necessary answer question available, hard find. Another direction give chatbot additional context current environment. example, aidea allows user interactively explore dataset help LLM. adds summary statistics dataset prompt LLM knows something data. Along lines, imagine writing chatbot help data import prompt include files current directory along first lines.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"structured-data-extraction","dir":"Articles","previous_headings":"Example uses","what":"Structured data extraction","title":"Getting started with ellmer","text":"LLMs often good extracting structured data unstructured text. can give traction analyse data previously unaccessible. example: Customer tickets GitHub issues: can use LLMs quick dirty sentiment analysis extracting specifically mentioned products summarising discussion bullet points. Geocoding: LLMs surprisingly good job geocoding, especially extracting addresses finding latitute/longitude cities. specialised tools better, using LLM makes easy get started. Recipes: ’ve extracted structured data baking cocktail recipes. data structured form can use R skills better understand recipes vary within cookbook look recipes use ingredients currently kitchen. even use shiny assistant help make techniques available anyone, just R users. Structured data extraction also works well images. ’s fastest cheapest way extract data makes really easy prototype ideas. example, maybe bunch scanned documents want index. can convert PDFs images (e.g. using {imagemagick}) use structured data extraction pull key details. Learn structured data extraction vignette(\"structure-data\").","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"programming","dir":"Articles","previous_headings":"Example uses","what":"Programming","title":"Getting started with ellmer","text":"LLMs can also useful solve general programming problems. example: Write detailed prompt explains update code use new version package. combine rstudioapi package allow user select code, transform , replace existing text. comprehensive example sort app pal, includes prompts automatically generating roxygen documentation blocks, updating testthat code 3rd edition, converting stop() abort() use cli::cli_abort(). automatically look documentation R function, include prompt make easier figure use specific function. can use LLMs explain code, even ask generate diagram. can ask LLM analyse code potential code smells security issues. can function time, explore entire source code package script prompt. use gh find unlabelled issues, extract text, ask LLM figure labels might appropriate. maybe LLM might able help people create better reprexes, simplify reprexes complicated? find useful LLM document function , even knowing ’s likely mostly incorrect. something react make much easier get started. ’re working code data another programming language, can ask LLM convert R code . Even ’s perfect, ’s still typically much faster everything .","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"miscellaneous","dir":"Articles","previous_headings":"","what":"Miscellaneous","title":"Getting started with ellmer","text":"finish ideas seem cool didn’t seem fit categories: Automatically generate alt text plots, using content_image_plot(). Analyse text statistical report look flaws statistical reasoning (e.g. misinterpreting p-values assuming causation correlation exists). Use existing company style guide generate brand.yaml specification automatically style reports, apps, dashboards plots match corporate style guide.","code":""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best practices","title":"Prompt design","text":"’s highly likely ’ll end writing long, possibly multi-page prompts. ensure success task, two recommendations. First, put prompt , separate file. Second, write prompts using markdown. reason use markdown ’s quite readable LLMs (humans), allows things like use headers divide prompt sections itemised lists enumerate multiple options. can see examples style prompt : https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_python.md https://github.com/jcheng5/py-sidebot/blob/main/prompt.md https://github.com/simonpcouch/pal/tree/main/inst/prompts https://github.com/cpsievert/aidea/blob/main/inst/app/prompt.md terms file names, one prompt project, call prompt.md. multiple prompts, give informative names like prompt-extract-metadata.md prompt-summarize-text.md. ’re writing package, put prompt(s) inst/prompts, otherwise ’s fine put project’s root directory. prompts going change time, ’d highly recommend commiting git repo. ensure can easily see changed, accidentally make mistake can easily roll back known good verison. prompt includes dynamic data, use ellmer::interpolate_file() intergrate prompt. interpolate_file() works like glue uses {{ }} instead { } make easier work JSON. iterate prompt, ’s good idea build small set challenging examples can regularly re-check latest version prompt. Currently ’ll need hand, hope eventually provide tools ’ll help little formally. Unfortunately, won’t see best practices action vignette since ’re keeping prompts short inline make easier grok ’s going .","code":""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"code-generation","dir":"Articles","previous_headings":"","what":"Code generation","title":"Prompt design","text":"Let’s explore prompt design simple code generation task: ’ll use chat_claude() problem experience best job generating code.","code":"question <- \"   How can I compute the mean and median of variables a, b, c, and so on,   all the way up to z, grouped by age and sex. \""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"basic-flavour","dir":"Articles","previous_headings":"Code generation","what":"Basic flavour","title":"Prompt design","text":"don’t provide system prompt, sometimes get answers different languages different styles R code: can ensure always get R code specific style providing system prompt: Note ’m using system prompt (defines general behaviour) user prompt (asks specific question). put content user prompt get similar results, think ’s helpful use cleanly divide general framing response specific questions ask. Since ’m mostly interested code, ask drop explanation sample data: course, want different style R code, just ask :","code":"chat <- chat_claude() chat$chat(question) #> Here's how to compute mean and median for variables a through z, grouped  #> by age and sex: #>  #> ```r #> # Using dplyr #> library(dplyr) #>  #> df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, list( #>     mean = ~mean(., na.rm = TRUE), #>     median = ~median(., na.rm = TRUE) #>   ))) #>  #> # Alternative base R approach #> aggregate(. ~ age + sex, data = df[,c(\"age\", \"sex\", letters[1:26])],  #>          FUN = function(x) c(mean = mean(x), median = median(x))) #> ``` #>  #> This will: #> 1. Group the data by age and sex #> 2. Calculate both mean and median for each variable a through z #> 3. Handle missing values with na.rm = TRUE #> 4. Return a dataframe with results for each age-sex combination #>  #> The output will have columns for age, sex, and mean/median values for  #> each variable. chat <- chat_claude(system_prompt = \"   You are an expert R programmer who prefers the tidyverse. \") chat$chat(question) #> Here's how you can compute means and medians for variables a through z,  #> grouped by age and sex, using tidyverse functions: #>  #> ```r #> library(tidyverse) #>  #> df %>% #>   group_by(age, sex) %>% #>   summarise(across( #>     .cols = a:z, #>     .fns = list( #>       mean = ~mean(., na.rm = TRUE), #>       median = ~median(., na.rm = TRUE) #>     ) #>   )) %>% #>   ungroup() #> ``` #>  #> This code will: #> 1. Group the data by age and sex #> 2. Use `across()` to apply both mean and median functions to all columns  #> from 'a' to 'z' #> 3. Handle missing values with `na.rm = TRUE` #> 4. The result will have columns named like `a_mean`, `a_median`,  #> `b_mean`, `b_median`, etc. #>  #> If you want to make the output more tidy (long format), you could add: #>  #> ```r #> df %>% #>   group_by(age, sex) %>% #>   summarise(across( #>     .cols = a:z, #>     .fns = list( #>       mean = ~mean(., na.rm = TRUE), #>       median = ~median(., na.rm = TRUE) #>     ) #>   )) %>% #>   ungroup() %>% #>   pivot_longer( #>     cols = -c(age, sex), #>     names_to = c(\"variable\", \"statistic\"), #>     names_pattern = \"(.*)_(.*)\", #>     values_to = \"value\" #>   ) #> ``` #>  #> This will give you a long format with columns: age, sex, variable,  #> statistic, and value. chat <- chat_claude(system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data. \") chat$chat(question) #> ```r #> library(tidyverse) #>  #> df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, list( #>     mean = ~mean(., na.rm = TRUE), #>     median = ~median(., na.rm = TRUE) #>   ))) %>% #>   ungroup() #> ``` chat <- chat_claude(system_prompt = \"   You are an expert R programmer who prefers data.table.   Just give me the code. I don't want any explanation or sample data. \") chat$chat(question) #> ```r #> library(data.table) #> DT[, lapply(.SD, function(x) list(mean = mean(x, na.rm = TRUE),  #>                                  median = median(x, na.rm = TRUE))),  #>    by = .(age, sex), #>    .SDcols = letters[1:26]] #> ``` chat <- chat_claude(system_prompt = \"   You are an expert R programmer who prefers base R.   Just give me the code. I don't want any explanation or sample data. \") chat$chat(question) #> ```r #> aggregate(cbind(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s,  #> t, u, v, w, x, y, z) ~ age + sex,  #>          data = data, #>          FUN = function(x) c(mean = mean(x, na.rm = TRUE),  #>                            median = median(x, na.rm = TRUE))) #> ```"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"be-explicit","dir":"Articles","previous_headings":"Code generation","what":"Be explicit","title":"Prompt design","text":"’s something output don’t like, try explicit. example, code isn’t styled quite ’d like , provide details want: still doesn’t yield exactly code ’d write, ’s pretty close. provide different prompt looking explanation code:","code":"chat <- chat_claude(system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data.    Follow the tidyverse style guide:   * Spread long function calls across multiple lines.   * Where needed, always indent function calls with two spaces.   * Only name arguments that are less commonly used.   * Always use double quotes for strings.   * Use the base pipe, `|>`, not the magrittr pipe `%>%`. \") chat$chat(question) #> ```r #> data |> #>   group_by(age, sex) |> #>   summarise( #>     across( #>       a:z, #>       list( #>         mean = mean, #>         median = median #>       ), #>       na.rm = TRUE #>     ) #>   ) #> ``` chat <- chat_claude(system_prompt = \"   You are an expert R teacher.   I am a new R user who wants to improve my programming skills.   Help me understand the code you produce by explaining each function call with   a brief comment. For more complicated calls, add documentation to each   argument. Just give me the code. I don't want any explanation or sample data. \") chat$chat(question) #> ```r #> # Create a vector of variable names from 'a' to 'z' #> vars <- letters #>  #> # Group data by age and sex, then compute mean and median for each  #> variable #> result <- data %>% #>   group_by(age, sex) %>% #>   summarise( #>     # Use across() to apply functions to multiple columns #>     across( #>       all_of(vars),  # Select variables a through z #>       list( #>         mean = ~mean(., na.rm = TRUE),    # Calculate mean, removing NAs #>         median = ~median(., na.rm = TRUE)  # Calculate median, removing  #> NAs #>       ) #>     ), #>     .groups = \"drop\"  # Drop grouping structure from result #>   ) #> ```"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"teach-it-about-new-features","dir":"Articles","previous_headings":"Code generation","what":"Teach it about new features","title":"Prompt design","text":"can imagine LLMs sort average internet given point time. means provide popular answers, tend reflect older coding styles (either new features aren’t index, older features much popular). want code use specific newer language features, might need provide examples :","code":"chat <- chat_claude(system_prompt = \"   You are an expert R programmer.   Just give me the code; no explanation in text.   Use the `.by` argument rather than `group_by()`.   dplyr 1.1.0 introduced per-operation grouping with the `.by` argument.   e.g., instead of:    transactions |>     group_by(company, year) |>     mutate(total = sum(revenue))    write this:   transactions |>     mutate(       total = sum(revenue),       .by = c(company, year)     ) \") chat$chat(question) #> ```r #> data |> #>   summarise( #>     across(a:z, list(mean = mean, median = median)), #>     .by = c(age, sex) #>   ) #> ```"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"structured-data","dir":"Articles","previous_headings":"","what":"Structured data","title":"Prompt design","text":"Providing rich set examples great way encourage output produce exactly want. known multi-shot prompting. ’ll work prompt designed extract structured data recipes, ideas apply many situations.","code":""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"getting-started","dir":"Articles","previous_headings":"Structured data","what":"Getting started","title":"Prompt design","text":"overall goal turn list ingredients, like following, nicely structured JSON can analyse R (e.g. compute total weight, scale recipe , convert units volumes weights). (isn’t ingredient list real recipe includes sampling styles encountered project.) don’t strong feelings data structure look like, can start loose prompt see get back. find useful pattern underspecified problems heavy lifting lies precisely defining problem want solve. Seeing LLM’s attempt create data structure gives something react , rather start blank page. (don’t know additional colour, “’re expert baker also loves JSON”, anything, like think helps LLM get right mindset nerdy baker.)","code":"ingredients <- \"   ¾ cup (150g) dark brown sugar   2 large eggs   ¾ cup (165g) sour cream   ½ cup (113g) unsalted butter, melted   1 teaspoon vanilla extract   ¾ teaspoon kosher salt   ⅓ cup (80ml) neutral oil   1½ cups (190g) all-purpose flour   150g plus 1½ teaspoons sugar \" instruct_json <- \"   You're an expert baker who also loves JSON. I am going to give you a list of   ingredients and your job is to return nicely structured JSON. Just return the   JSON and no other commentary. \"  chat <- chat_openai(instruct_json) #> Using model = \"gpt-4o\". chat$chat(ingredients) #> ```json #> { #>   \"ingredients\": [ #>     { #>       \"quantity\": \"¾ cup\", #>       \"weight\": \"150g\", #>       \"item\": \"dark brown sugar\" #>     }, #>     { #>       \"quantity\": \"2\", #>       \"item\": \"large eggs\" #>     }, #>     { #>       \"quantity\": \"¾ cup\", #>       \"weight\": \"165g\", #>       \"item\": \"sour cream\" #>     }, #>     { #>       \"quantity\": \"½ cup\", #>       \"weight\": \"113g\", #>       \"item\": \"unsalted butter, melted\" #>     }, #>     { #>       \"quantity\": \"1 teaspoon\", #>       \"item\": \"vanilla extract\" #>     }, #>     { #>       \"quantity\": \"¾ teaspoon\", #>       \"item\": \"kosher salt\" #>     }, #>     { #>       \"quantity\": \"⅓ cup\", #>       \"volume\": \"80ml\", #>       \"item\": \"neutral oil\" #>     }, #>     { #>       \"quantity\": \"1½ cups\", #>       \"weight\": \"190g\", #>       \"item\": \"all-purpose flour\" #>     }, #>     { #>       \"weight\": \"150g\", #>       \"item\": \"sugar\" #>     }, #>     { #>       \"quantity\": \"1½ teaspoons\", #>       \"item\": \"sugar\" #>     } #>   ] #> } #> ```"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"provide-examples","dir":"Articles","previous_headings":"Structured data","what":"Provide examples","title":"Prompt design","text":"isn’t bad start, prefer cook weight want see volumes weight isn’t available provide couple examples ’m looking . pleasantly suprised can provide input output examples loose format. Just providing examples seems work remarkably well. found useful also include description examples trying accomplish. ’m sure helps LLM , certainly makes easier understand organisation whole prompt check ’ve covered key pieces ’m interested . structure also allows give LLMs hint want multiple ingredients stored, .e. JSON array. iterated prompt, looking results different recipes get sense LLM getting wrong. Much felt like waws iterating understanding problem didn’t start knowing exactly wanted data. example, started didn’t really think various ways ingredients specified. later analysis, always want quantities number, even originally fractions, units aren’t precise (like pinch). made realise ingredients unitless. might want take look full prompt see ended .","code":"instruct_weight <- r\"(   Here are some examples of the sort of output I'm looking for:    ¾ cup (150g) dark brown sugar   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}    ⅓ cup (80ml) neutral oil   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}    2 t ground cinnamon   {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"} )\"  chat <- chat_openai(c(instruct_json, instruct_weight)) #> Using model = \"gpt-4o\". chat$chat(ingredients) #> ```json #> [ #>   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"large eggs\", \"quantity\": 2, \"unit\": \"count\"}, #>   {\"name\": \"sour cream\", \"quantity\": 165, \"unit\": \"g\"}, #>   {\"name\": \"unsalted butter, melted\", \"quantity\": 113, \"unit\": \"g\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\"}, #>   {\"name\": \"kosher salt\", \"quantity\": 0.75, \"unit\": \"teaspoon\"}, #>   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 190, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 1.5, \"unit\": \"teaspoon\"} #> ] #> ``` instruct_weight <- r\"(   * If an ingredient has both weight and volume, extract only the weight:    ¾ cup (150g) dark brown sugar   [     {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}   ]  * If an ingredient only lists a volume, extract that.    2 t ground cinnamon   ⅓ cup (80ml) neutral oil   [     {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"},     {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}   ] )\" instruct_unit <- r\"( * If the unit uses a fraction, convert it to a decimal.    ⅓ cup sugar   ½ teaspoon salt   [     {\"name\": \"dark brown sugar\", \"quantity\": 0.33, \"unit\": \"cup\"},     {\"name\": \"salt\", \"quantity\": 0.5, \"unit\": \"teaspoon\"}   ]  * Quantities are always numbers    pinch of kosher salt   [     {\"name\": \"kosher salt\", \"quantity\": 1, \"unit\": \"pinch\"}   ]  * Some ingredients don't have a unit.   2 eggs   1 lime   1 apple   [     {\"name\": \"egg\", \"quantity\": 2},     {\"name\": \"lime\", \"quantity\": 1},     {\"name\", \"apple\", \"quantity\": 1}   ] )\""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"structured-data-1","dir":"Articles","previous_headings":"Structured data","what":"Structured data","title":"Prompt design","text":"Now ’ve iterated get data structure like, seems useful formalise tell LLM exactly ’m looking dealing structured data. guarantees LLM return JSON, JSON fields expect, ellmer convert R data structure.","code":"type_ingredient <- type_object(   name = type_string(\"Ingredient name\"),   quantity = type_number(),   unit = type_string(\"Unit of measurement\") )  type_ingredients <- type_array(items = type_ingredient)  chat <- chat_openai(c(instruct_json, instruct_weight)) #> Using model = \"gpt-4o\". data <- chat$extract_data(ingredients, type = type_object(ingredients = type_ingredients)) do.call(rbind, lapply(data$ingredients, as.data.frame)) #>                              X[[i]] #> name.1             dark brown sugar #> name.2                   large eggs #> name.3                   sour cream #> name.4      unsalted butter, melted #> name.5              vanilla extract #> name.6                  kosher salt #> name.7                  neutral oil #> name.8            all-purpose flour #> name.9                        sugar #> name.10                       sugar #> quantity.1                      150 #> quantity.2                        2 #> quantity.3                      165 #> quantity.4                      113 #> quantity.5                        1 #> quantity.6                     0.75 #> quantity.7                       80 #> quantity.8                      190 #> quantity.9                      150 #> quantity.10                     1.5 #> unit.1                            g #> unit.2                              #> unit.3                            g #> unit.4                            g #> unit.5                     teaspoon #> unit.6                     teaspoon #> unit.7                           ml #> unit.8                            g #> unit.9                            g #> unit.10                    teaspoon"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"capturing-raw-input","dir":"Articles","previous_headings":"Structured data","what":"Capturing raw input","title":"Prompt design","text":"One thing ’d next time also include raw ingredient names output. doesn’t make much difference simple example makes much easier align input output start developing automated measures well prompt . think particularly important ’re working even less structured text. example, imagine text: Including input text output makes easier see ’s good job: ran writing vignette, seemed working weight ingredients specified volume, even though prompt specifically asks . may suggest need broaden examples.","code":"instruct_weight_input <- r\"(   * If an ingredient has both weight and volume, extract only the weight:      ¾ cup (150g) dark brown sugar     [       {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\", \"input\": \"¾ cup (150g) dark brown sugar\"}     ]    * If an ingredient only lists a volume, extract that.      2 t ground cinnamon     ⅓ cup (80ml) neutral oil     [       {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\", \"input\": \"2 t ground cinnamon\"},       {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\", \"input\": \"⅓ cup (80ml) neutral oil\"}     ] )\" recipe <- r\"(   In a large bowl, cream together one cup of softened unsalted butter and a   quarter cup of white sugar until smooth. Beat in an egg and 1 teaspoon of   vanilla extract. Gradually stir in 2 cups of all-purpose flour until the   dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop   spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C)   for 10-12 minutes, or until the edges are lightly browned. Let the cookies   cool on the baking sheet for a few minutes before transferring to a wire   rack to cool completely. Enjoy! )\" chat <- chat_openai(c(instruct_json, instruct_weight_input)) #> Using model = \"gpt-4o\". chat$chat(recipe) #> ```json #> [ #>   {\"name\": \"unsalted butter\", \"quantity\": 1, \"unit\": \"cup\", \"input\": \"one #> cup of softened unsalted butter\"}, #>   {\"name\": \"white sugar\", \"quantity\": 0.25, \"unit\": \"cup\", \"input\": \"a  #> quarter cup of white sugar\"}, #>   {\"name\": \"egg\", \"quantity\": 1, \"unit\": \"piece\", \"input\": \"an egg\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\", \"input\": #> \"1 teaspoon of vanilla extract\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 2, \"unit\": \"cup\", \"input\": \"2 #> cups of all-purpose flour\"}, #>   {\"name\": \"semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\",  #> \"input\": \"1 cup of semisweet chocolate chips\"} #> ] #> ```"},{"path":[]},{"path":"https://ellmer.tidyverse.org/articles/streaming-async.html","id":"streaming-results","dir":"Articles","previous_headings":"","what":"Streaming results","title":"Streaming and async APIs","text":"chat() method return results entire response received. (can print streaming results console, returns result response complete.) want process response arrives, can use stream() method. may useful want display response realtime, somewhere R console (like writing file, HTTP response, Shiny chat window); want manipulate response displaying , without giving immediacy streaming. stream() method returns generator coro package, can loop process response arrives.","code":"stream <- chat$stream(\"What are some common uses of R?\") coro::loop(for (chunk in stream) {   cat(toupper(chunk)) }) #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING COMPLEX STATISTICAL TESTS AND ANALYSES. #>  2. **DATA VISUALIZATION**: CREATING GRAPHS, CHARTS, AND PLOTS USING PACKAGES LIKE  GGPLOT2. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR AND TIDYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS WITH LIBRARIES LIKE CARET AND #>  RANDOMFOREST. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA AND GENOMIC STUDIES. #>  6. **ECONOMETRICS**: PERFORMING ECONOMIC DATA ANALYSIS AND MODELING. #>  7. **REPORTING**: GENERATING DYNAMIC REPORTS AND DASHBOARDS WITH R MARKDOWN. #>  8. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA AND FORECASTING. #> #>  THESE USES MAKE R A POWERFUL TOOL FOR DATA SCIENTISTS, STATISTICIANS, AND RESEARCHERS."},{"path":"https://ellmer.tidyverse.org/articles/streaming-async.html","id":"async-usage","dir":"Articles","previous_headings":"","what":"Async usage","title":"Streaming and async APIs","text":"ellmer also supports async usage, useful want run multiple chat sessions concurrently. primarily useful Shiny applications, using methods described block Shiny app users duration response. use async chat, instead chat()/stream(), call chat_async()/stream_async(). _async variants take arguments construction, return promises instead actual response. Remember chat objects stateful, maintaining conversation history interact . Note means doesn’t make sense issue multiple chat/stream operations chat object concurrently, conversation history become corrupted interleaved conversation fragments. need run multiple chat sessions concurrently, create multiple chat objects.","code":""},{"path":"https://ellmer.tidyverse.org/articles/streaming-async.html","id":"asynchronous-chat","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous chat","title":"Streaming and async APIs","text":"asynchronous, non-streaming chat, use chat() method , handle result promise instead string. TODO: Shiny example","code":"library(promises)  chat$chat_async(\"How's your day going?\") %...>% print() #> I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions you have."},{"path":"https://ellmer.tidyverse.org/articles/streaming-async.html","id":"asynchronous-streaming","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous streaming","title":"Streaming and async APIs","text":"asynchronous streaming, use stream() method , result async generator coro package. regular generator, except instead giving strings, gives promises resolve strings. Async generators advanced, require good understanding asynchronous programming R. also way present streaming results Shiny without blocking users. Fortunately, Shiny soon chat components make easier, can simply hand result stream_async() chat output.","code":"stream <- chat$stream_async(\"What are some common uses of R?\") coro::async(function() {   for (chunk in await_each(stream)) {     cat(toupper(chunk))   } })() #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING VARIOUS STATISTICAL TESTS AND MODELS. #>  2. **DATA VISUALIZATION**: CREATING PLOTS AND GRAPHS TO VISUALIZE DATA. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS AND ALGORITHMS. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA, ESPECIALLY IN GENOMICS. #>  6. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA FOR TRENDS AND FORECASTS. #>  7. **REPORT GENERATION**: CREATING DYNAMIC REPORTS WITH R MARKDOWN. #>  8. **GEOSPATIAL ANALYSIS**: MAPPING AND ANALYZING GEOGRAPHIC DATA."},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"structured-data-basics","dir":"Articles","previous_headings":"","what":"Structured data basics","title":"Structured data","text":"extract structured data call $extract_data() method instead $chat() method. ’ll also need define type specification describes structure data want (shortly). ’s simple example extracts two specific values string: basic idea works images :","code":"chat <- chat_openai() #> Using model = \"gpt-4o\". chat$extract_data(   \"My name is Susan and I'm 13 years old\",   type = type_object(     age = type_number(),     name = type_string()   ) ) #> $age #> [1] 13 #>  #> $name #> [1] \"Susan\" chat$extract_data(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   type = type_object(     primary_shape = type_string(),     primary_colour = type_string()   ) ) #> $primary_shape #> [1] \"oval and a letter R\" #>  #> $primary_colour #> [1] \"silver and blue\""},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"data-types-basics","dir":"Articles","previous_headings":"","what":"Data types basics","title":"Structured data","text":"define desired type specification (also known schema), use type_() functions. (might already familiar ’ve done function calling, discussed vignette(\"function-calling\")). type functions can divided three main groups: Scalars represent single values, five types: type_boolean(), type_integer(), type_number(), type_string(), type_enum(), representing single logical, integer, double, string, factor value respectively. Arrays represent number values type created type_array(). must always supply item argument specifies type individual element. Arrays scalars similar R’s atomic vectors: can also arrays arrays arrays objects, closely resemble lists well defined structures: Objects represent collection named values created type_object(). Objects can contain number scalars, arrays, objects. similar named lists R. Using type specifications ensures LLM return JSON. ellmer goes one step convert results natural R representation. currently converts arrays boolean, integers, numbers, strings logical, integer, numeric, character vectors, arrays objects data frames. can opt-get plain lists instead setting convert = FALSE $extract_data(). well definition types, need provide LLM information actually want. purpose first argument, description, string describes data want. good place ask nicely attributes ’ll like value possess (e.g. minimum maximum values, date formats, …). aren’t guaranteed requests honoured, LLM usually make best effort . Now ’ll dive examples coming back talk data types details.","code":"type_logical_vector <- type_array(items = type_boolean()) type_integer_vector <- type_array(items = type_integer()) type_double_vector <- type_array(items = type_number()) type_character_vector <- type_array(items = type_string()) list_of_integers <- type_array(items = type_integer_vector) type_person <- type_object(   name = type_string(),   age = type_integer(),   hobbies = type_array(items = type_string()) ) type_type_person <- type_object(   \"A person\",   name = type_string(\"Name\"),   age = type_integer(\"Age, in years.\"),   hobbies = type_array(     \"List of hobbies. Should be exclusive and brief.\",     items = type_string()   ) )"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Structured data","text":"following examples closely inspired Claude documentation hint ways can use structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-1-article-summarisation","dir":"Articles","previous_headings":"Examples","what":"Example 1: Article summarisation","title":"Structured data","text":"","code":"text <- readLines(system.file(\"examples/third-party-testing.txt\", package = \"ellmer\")) # url <- \"https://www.anthropic.com/news/third-party-testing\" # html <- rvest::read_html(url) # text <- rvest::html_text2(rvest::html_element(html, \"article\"))  type_summary <- type_object(   \"Summary of the article.\",   author = type_string(\"Name of the article author\"),   topics = type_array(     'Array of topics, e.g. [\"tech\", \"politics\"]. Should be as specific as possible, and can overlap.',     type_string(),   ),   summary = type_string(\"Summary of the article. One or two paragraphs max\"),   coherence = type_integer(\"Coherence of the article's key points, 0-100 (inclusive)\"),   persuasion = type_number(\"Article's persuasion score, 0.0-1.0 (inclusive)\") )  chat <- chat_openai() #> Using model = \"gpt-4o\". data <- chat$extract_data(text, type = type_summary) cat(data$summary) #> This article from Anthropic argues for the necessity of third-party testing as a key component of AI policy to address the challenges posed by frontier AI systems. These systems, such as large-scale generative models, possess immense capabilities that, if left unchecked, can lead to misuses or accidents. The article stresses that a robust, third-party testing regime is vital for instilling trust in AI systems, ensuring safety, and managing risks, but it must be carefully designed to avoid disadvantaging smaller enterprises and must adapt to future advancements in AI capabilities. The article outlines the proposed structure and implementation of such a regime, involving cooperation across industry, government, and academia, with an emphasis on avoiding regulatory capture and maintaining an open dissemination of AI while ensuring safety.  str(data) #> List of 5 #>  $ author    : chr \"Anthropic\" #>  $ topics    : chr [1:6] \"AI Safety\" \"Regulatory Policy\" \"Third-party Testing\" \"Generative AI\" ... #>  $ summary   : chr \"This article from Anthropic argues for the necessity of third-party testing as a key component of AI policy to \"| __truncated__ #>  $ coherence : int 95 #>  $ persuasion: num 0.85"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-2-named-entity-recognition","dir":"Articles","previous_headings":"Examples","what":"Example 2: Named entity recognition","title":"Structured data","text":"","code":"text <- \"   John works at Google in New York. He met with Sarah, the CEO of   Acme Inc., last week in San Francisco. \"  type_named_entity <- type_object(   name = type_string(\"The extracted entity name.\"),   type = type_enum(\"The entity type\", c(\"person\", \"location\", \"organization\")),   context = type_string(\"The context in which the entity appears in the text.\") ) type_named_entities <- type_array(items = type_named_entity)  chat <- chat_openai() #> Using model = \"gpt-4o\". chat$extract_data(text, type = type_named_entities) #>            name         type                                     context #> 1          John       person Mentioned as working at Google in New York. #> 2        Google organization               The company where John works. #> 3      New York     location              The location where John works. #> 4         Sarah       person           Mentioned as the CEO of Acme Inc. #> 5     Acme Inc. organization         The company where Sarah is the CEO. #> 6 San Francisco     location          The location where John met Sarah."},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-3-sentiment-analysis","dir":"Articles","previous_headings":"Examples","what":"Example 3: Sentiment analysis","title":"Structured data","text":"Note ’ve asked nicely scores sum 1, example (least ran code), ’s guaranteed.","code":"text <- \"   The product was okay, but the customer service was terrible. I probably   won't buy from them again. \"  type_sentiment <- type_object(   \"Extract the sentiment scores of a given text. Sentiment scores should sum to 1.\",   positive_score = type_number(\"Positive sentiment score, ranging from 0.0 to 1.0.\"),   negative_score = type_number(\"Negative sentiment score, ranging from 0.0 to 1.0.\"),   neutral_score = type_number(\"Neutral sentiment score, ranging from 0.0 to 1.0.\") )  chat <- chat_openai() #> Using model = \"gpt-4o\". str(chat$extract_data(text, type = type_sentiment)) #> List of 3 #>  $ positive_score: num 0.1 #>  $ negative_score: num 0.7 #>  $ neutral_score : num 0.2"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-4-text-classification","dir":"Articles","previous_headings":"Examples","what":"Example 4: Text classification","title":"Structured data","text":"","code":"text <- \"The new quantum computing breakthrough could revolutionize the tech industry.\"  type_classification <- type_array(   \"Array of classification results. The scores should sum to 1.\",   type_object(     name = type_enum(       \"The category name\",       values = c(         \"Politics\",         \"Sports\",         \"Technology\",         \"Entertainment\",         \"Business\",         \"Other\"       )     ),     score = type_number(       \"The classification score for the category, ranging from 0.0 to 1.0.\"     )   ) )  chat <- chat_openai() #> Using model = \"gpt-4o\". data <- chat$extract_data(text, type = type_classification) data #>            name score #> 1    Technology 0.700 #> 2      Business 0.200 #> 3         Other 0.050 #> 4 Entertainment 0.025 #> 5      Politics 0.020 #> 6        Sports 0.005"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-5-working-with-unknown-keys","dir":"Articles","previous_headings":"Examples","what":"Example 5: Working with unknown keys","title":"Structured data","text":"examples works Claude, GPT Gemini, Claude supports adding arbitrary additional properties.","code":"type_characteristics <- type_object(   \"All characteristics\",   .additional_properties = TRUE )  prompt <- \"   Given a description of a character, your task is to extract all the characteristics of that character.    <description>   The man is tall, with a beard and a scar on his left cheek. He has a deep voice and wears a black leather jacket.   <\/description> \"  chat <- chat_claude() str(chat$extract_data(prompt, type = type_characteristics)) #>  list()"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-6-extracting-data-from-an-image","dir":"Articles","previous_headings":"Examples","what":"Example 6: Extracting data from an image","title":"Structured data","text":"example comes Dan Nguyen can see interesting applications link. goal extract structured data screenshot: Even without descriptions, ChatGPT pretty well:","code":"type_asset <- type_object(   assert_name = type_string(),   owner = type_string(),   location = type_string(),   asset_value_low = type_integer(),   asset_value_high = type_integer(),   income_type = type_string(),   income_low = type_integer(),   income_high = type_integer(),   tx_gt_1000 = type_boolean() ) type_assets <- type_array(items = type_asset)  chat <- chat_openai() #> Using model = \"gpt-4o\". image <- content_image_file(\"congressional-assets.png\") data <- chat$extract_data(image, type = type_assets) data #>                                 assert_name owner #> 1  11 Zinfandel Lane - Home & Vineyard [RP]    JT #> 2 25 Point Lobos - Commercial Property [RP]    SP #>                              location asset_value_low asset_value_high #> 1             St. Helena/Napa, CA, US         5000001         25000000 #> 2 San Francisco/San Francisco, CA, US         5000001         25000000 #>   income_type income_low income_high tx_gt_1000 #> 1 Grape Sales     100001     1000000       TRUE #> 2        Rent     100001     1000000      FALSE"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"advanced-data-types","dir":"Articles","previous_headings":"","what":"Advanced data types","title":"Structured data","text":"Now ’ve seen examples, ’s time get specifics data type declarations.","code":""},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"required-vs-optional","dir":"Articles","previous_headings":"Advanced data types","what":"Required vs optional","title":"Structured data","text":"default, components object required. want make optional, set required = FALSE. good idea don’t think text always contain required fields LLMs may hallucinate data order fulfill spec. example, LLM hallucinates date even though isn’t one text: Note ’ve used explict prompt . example, found generated better results, ’s useful place put additional instructions. let LLM know fields optional, ’ll instead return NULL missing fields:","code":"type_article <- type_object(   \"Information about an article written in markdown\",   title = type_string(\"Article title\"),   author = type_string(\"Name of the author\"),   date = type_string(\"Date written in YYYY-MM-DD format.\") )  prompt <- \"   Extract data from the following text:    <text>   # Structured Data   By Hadley Wickham    When using an LLM to extract data from text or images, you can ask the chatbot to nicely format it, in JSON or any other format that you like.   <\/text> \"  chat <- chat_openai() #> Using model = \"gpt-4o\". chat$extract_data(prompt, type = type_article) #> $title #> [1] \"Structured Data\" #>  #> $author #> [1] \"Hadley Wickham\" #>  #> $date #> [1] \"\" str(data) #> 'data.frame':    2 obs. of  9 variables: #>  $ assert_name     : chr  \"11 Zinfandel Lane - Home & Vineyard [RP]\" \"25 Point Lobos - Commercial Property [RP]\" #>  $ owner           : chr  \"JT\" \"SP\" #>  $ location        : chr  \"St. Helena/Napa, CA, US\" \"San Francisco/San Francisco, CA, US\" #>  $ asset_value_low : int  5000001 5000001 #>  $ asset_value_high: int  25000000 25000000 #>  $ income_type     : chr  \"Grape Sales\" \"Rent\" #>  $ income_low      : int  100001 100001 #>  $ income_high     : int  1000000 1000000 #>  $ tx_gt_1000      : logi  TRUE FALSE type_article <- type_object(   \"Information about an article written in markdown\",   title = type_string(\"Article title\", required = FALSE),   author = type_string(\"Name of the author\", required = FALSE),   date = type_string(\"Date written in YYYY-MM-DD format.\", required = FALSE) ) chat$extract_data(prompt, type = type_article) #> $title #> [1] \"Structured Data\" #>  #> $author #> [1] \"Hadley Wickham\" #>  #> $date #> NULL"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"data-frames","dir":"Articles","previous_headings":"Advanced data types","what":"Data frames","title":"Structured data","text":"want define data frame like object, might tempted create definition similar R uses: object (.e. named list) containing multiple vectors (.e. arrays): however, quite right becuase ’s way specify array length. Instead need turn data structure “inside ”, instead create array objects: ’re familiar terms row-oriented column-oriented data frames, idea. Since language don’t possess vectorisation like R, row-oriented structures tend much common wild.","code":"type_my_df <- type_object(   name = type_array(items = type_string()),   age = type_array(items = type_integer()),   height = type_array(items = type_number()),   weight = type_array(items = type_number()) ) type_my_df <- type_array(   items = type_object(     name = type_string(),     age = type_integer(),     height = type_number(),     weight = type_number()   ) )"},{"path":[]},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tool/function calling","text":"One interesting aspects modern chat models ability make use external tools defined caller. making chat request chat model, caller advertises one tools (defined function name, description, list expected arguments), chat model can choose respond one “tool calls”. tool calls requests chat model caller execute function given arguments; caller expected execute functions “return” results submitting another chat request conversation far, plus results. chat model can use results formulating response, , may decide make additional tool calls. Note chat model directly execute external tools! makes requests caller execute . ’s easy think tool calling might work like : fact works like : value chat model brings helping execution, knowing makes sense call tool, values pass arguments, use results formulating response.","code":"library(ellmer)"},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"motivating-example","dir":"Articles","previous_headings":"Introduction","what":"Motivating example","title":"Tool/function calling","text":"Let’s take look example really need external tool. Chat models generally know current time, makes questions like impossible. Unfortunately, example run September 18, 2024. Let’s give chat model ability determine current time try .","code":"chat <- chat_openai(model = \"gpt-4o\") chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") #> Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. To determine how long ago that #> was from the current year of 2023, we can calculate the difference in years, months, and days. #> #> From July 20, 1969, to July 20, 2023, is exactly 54 years. If today's date is after July 20, 2023, you #> would add the additional time since then. If it is before, you would consider slightly less than 54 #> years. #> #> As of right now, can you confirm the current date so we can calculate the precise duration?"},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"defining-a-tool-function","dir":"Articles","previous_headings":"Introduction","what":"Defining a tool function","title":"Tool/function calling","text":"first thing ’ll define R function returns current time. tool. Note ’ve gone trouble creating roxygen2 comments. important step help model use tool correctly! Let’s test :","code":"#' Gets the current time in the given time zone. #' #' @param tz The time zone to get the current time in. #' @return The current time in the given time zone. get_current_time <- function(tz = \"UTC\") {   format(Sys.time(), tz = tz, usetz = TRUE) } get_current_time() #> [1] \"2024-09-18 17:47:14 UTC\""},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"registering-tools","dir":"Articles","previous_headings":"Introduction","what":"Registering tools","title":"Tool/function calling","text":"Now need tell chat object get_current_time function. creating registering tool: fair amount code write, even simple function get_current_time. Fortunately, don’t write hand! generated register_tool call calling create_tool_metadata(get_current_time), printed code console. create_tool_metadata() works passing function’s signature documentation GPT-4o, asking generate register_tool call . Note create_tool_metadata() may create perfect results, must review generated code using . huge time-saver nonetheless, removes tedious boilerplate generation ’d otherwise.","code":"chat <- chat_openai(model = \"gpt-4o\")  chat$register_tool(tool(   get_current_time,   \"Gets the current time in the given time zone.\",   tz = type_string(     \"The time zone to get the current time in. Defaults to `\\\"UTC\\\"`.\",     required = FALSE   ) ))"},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"using-the-tool","dir":"Articles","previous_headings":"Introduction","what":"Using the tool","title":"Tool/function calling","text":"’s need ! Let’s retry query: ’s correct! Without guidance, chat model decided call tool function successfully used result formulating response. (Full disclosure: originally tried example default model gpt-4o-mini got tool calling right date math wrong, hence explicit model=\"gpt-4o\".) tool example extremely simple, can imagine much interesting things tool functions: calling APIs, reading writing database, kicking complex simulation, even calling complementary GenAI model (like image generator). using ellmer Shiny app, use tools set reactive values, setting chain reactive updates.","code":"chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") #> Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. #> #> To calculate the time elapsed from that moment until the current time (September 18, 2024, 17:47:19 #> UTC), we need to break it down. #> #> 1. From July 20, 1969, 20:17 UTC to July 20, 2024, 20:17 UTC is exactly 55 years. #> 2. From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, we need to further break down: #> #>    - From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, which is: #>      - 1 full month (August) #>      - 30 – 20 = 10 days of July #>      - 18 days of September until 17:47:19 UTC #> #> So, in detail: #>    - 55 years #>    - 1 month #>    - 28 days #>    - From July 20, 2024, 20:17 UTC to July 20, 2024, 17:47:19 UTC: 23 hours, 30 minutes, and 19 seconds #> #> Time Total: #> - 55 years #> - 1 month #> - 28 days #> - 23 hours #> - 30 minutes #> - 19 seconds #> #> This is the exact time that has elapsed since Neil Armstrong's historic touchdown on the moon."},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"tool-limitations","dir":"Articles","previous_headings":"Introduction","what":"Tool limitations","title":"Tool/function calling","text":"Remember tool arguments come chat model, tool results returned chat model. means simple, {jsonlite} compatible data types can used inputs outputs. ’s highly recommended stick strings/character, numbers, booleans/logical, null, named unnamed lists types. can forget using functions, environments, external pointers, R6 classes, complex R objects arguments return values. Returning data frames seems work OK, although careful return much data, counts tokens (.e., count context window limit also cost money).","code":""},{"path":"https://ellmer.tidyverse.org/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hadley Wickham. Author, maintainer. Joe Cheng. Author. . Copyright holder, funder.","code":""},{"path":"https://ellmer.tidyverse.org/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wickham H, Cheng J (2024). ellmer: Chat Large Language Models. R package version 0.0.0.9000, https://github.com/tidyverse/ellmer, https://ellmer.tidyverse.org.","code":"@Manual{,   title = {ellmer: Chat with Large Language Models},   author = {Hadley Wickham and Joe Cheng},   year = {2024},   note = {R package version 0.0.0.9000, https://github.com/tidyverse/ellmer},   url = {https://ellmer.tidyverse.org}, }"},{"path":"https://ellmer.tidyverse.org/index.html","id":"ellmer-","dir":"","previous_headings":"","what":"Chat with Large Language Models","title":"Chat with Large Language Models","text":"ellmer makes easy use large language models (LLM) R. supports wide variety LLM providers implements rich set features including streaming outputs, tool/function calling, structured data extraction, . (Looking something similar ellmer python? Check chatlas!)","code":""},{"path":"https://ellmer.tidyverse.org/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Chat with Large Language Models","text":"can install ellmer CRAN :","code":"install.packages(\"ellmer\")"},{"path":"https://ellmer.tidyverse.org/index.html","id":"providers","dir":"","previous_headings":"","what":"Providers","title":"Chat with Large Language Models","text":"ellmer supports wide variety model providers: Anthropic’s Claude: chat_claude(). AWS Bedrock: chat_bedrock(). Azure OpenAI: chat_azure(). Databricks: chat_databricks(). GitHub model marketplace: chat_github(). Google Gemini: chat_gemini(). Groq: chat_groq(). Ollama: chat_ollama(). OpenAI: chat_openai(). perplexity.ai: chat_perplexity(). Snowflake Cortex: chat_cortex(). VLLM: chat_vllm().","code":""},{"path":"https://ellmer.tidyverse.org/index.html","id":"model-choice","dir":"","previous_headings":"","what":"Model choice","title":"Chat with Large Language Models","text":"’re using ellmer inside organisation, ’ll limited department allows, likely one provided big cloud provider, e.g. chat_azure(), chat_bedrock(), chat_databricks(), chat_snowflake(). ’re using ellmer exploration, ’ll lot freedom, recommendations help get started: chat_openai() chat_claude() good places start. chat_openai() defaults GPT-4o, can use model = \"gpt-4o-mini\" cheaper, lower-quality model, model = \"o1-mini\" complex reasoning. chat_claude() also good; defaults Claude 3.5 Sonnet, found particularly good writing code. chat_gemini() great large prompts much larger context window models. allows 1 million tokens, compared Claude 3.5 Sonnet’s 200k GPT-4o’s 128k. chat_ollama(), uses Ollama, allows run models computer. biggest models can run locally aren’t good state art hosted models, don’t share data effectively free.","code":""},{"path":"https://ellmer.tidyverse.org/index.html","id":"using-ellmer","dir":"","previous_headings":"","what":"Using ellmer","title":"Chat with Large Language Models","text":"can work ellmer several different ways, depending whether working interactively programmatically. start creating new chat object: Chat objects stateful R6 objects: retain context conversation, new query builds previous ones. call methods $.","code":"library(ellmer)  chat <- chat_openai(   model = \"gpt-4o-mini\",   system_prompt = \"You are a friendly but terse assistant.\", )"},{"path":"https://ellmer.tidyverse.org/index.html","id":"interactive-chat-console","dir":"","previous_headings":"Using ellmer","what":"Interactive chat console","title":"Chat with Large Language Models","text":"interactive least programmatic way using ellmer chat directly R console browser live_console(chat) live_browser(): Keep mind chat object retains state, enter chat console, previous interactions chat object still part conversation, interactions chat console persist exit back R prompt. true regardless chat function use.","code":"live_console(chat) #> ╔════════════════════════════════════════════════════════╗ #> ║  Entering chat console. Use \"\"\" for multi-line input.  ║ #> ║  Press Ctrl+C to quit.                                 ║ #> ╚════════════════════════════════════════════════════════╝ #> >>> Who were the original creators of R? #> R was originally created by Ross Ihaka and Robert Gentleman at the University of #> Auckland, New Zealand. #> #> >>> When was that? #> R was initially released in 1995. Development began a few years prior to that, #> in the early 1990s."},{"path":"https://ellmer.tidyverse.org/index.html","id":"interactive-method-call","dir":"","previous_headings":"Using ellmer","what":"Interactive method call","title":"Chat with Large Language Models","text":"second interactive way chat call chat() method: initialize chat object global environment, chat method stream response console. entire response received, ’s also (invisibly) returned character vector. useful want see response arrives, don’t want enter chat console. want ask question image, can pass one additional input arguments using content_image_file() /content_image_url():","code":"chat$chat(\"What preceding languages most influenced R?\") #> R was primarily influenced by the S programming language, particularly S-PLUS. #> Other languages that had an impact include Scheme and various data analysis #> languages. chat$chat(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   \"Can you explain this logo?\" ) #> The logo of R features a stylized letter \"R\" in blue, enclosed in an oval #> shape that resembles the letter \"O,\" signifying the programming language's #> name. The design conveys a modern and professional look, reflecting its use #> in statistical computing and data analysis. The blue color often represents #> trust and reliability, which aligns with R's role in data science."},{"path":"https://ellmer.tidyverse.org/index.html","id":"programmatic-chat","dir":"","previous_headings":"Using ellmer","what":"Programmatic chat","title":"Chat with Large Language Models","text":"programmatic way chat create chat object inside function. , live streaming automatically suppressed $chat() returns result string: needed, can manually control behaviour echo argument. useful programming ellmer result either intended human consumption want process response displaying .","code":"my_function <- function() {   chat <- chat_openai(     model = \"gpt-4o-mini\",     system_prompt = \"You are a friendly but terse assistant.\",   )   chat$chat(\"Is R a functional programming language?\") } my_function() #> [1] \"Yes, R supports functional programming concepts. It allows functions to #> be first-class objects, supports higher-order functions, and encourages the #> use of functions as core components of code. However, it also supports #> procedural and object-oriented programming styles.\""},{"path":"https://ellmer.tidyverse.org/index.html","id":"learning-more","dir":"","previous_headings":"","what":"Learning more","title":"Chat with Large Language Models","text":"ellmer comes bunch vignettes help learn : Learn key vocabulary see example use cases vignette(\"ellmer\"). Learn design prompt vignette(\"prompt-design\"). Learn tool/function calling vignette(\"tool-calling\"). Learn extract structured data vignette(\"structured-data\"). Learn streaming async APIs vignette(\"streaming-async\").","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":null,"dir":"Reference","previous_headings":"","what":"A chat — Chat","title":"A chat — Chat","text":"Chat sequence sequence user assistant Turns sent specific Provider. Chat mutable R6 object takes care managing state associated chat; .e. records messages send server, messages receive back. register tool (.e. R function assistant can call behalf), also takes care tool loop. generally create object , instead call chat_openai() friends instead.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A chat — Chat","text":"Chat object","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"A chat — Chat","text":"Chat$new() Chat$get_turns() Chat$set_turns() Chat$get_system_prompt() Chat$set_system_prompt() Chat$tokens() Chat$last_turn() Chat$chat() Chat$extract_data() Chat$extract_data_async() Chat$chat_async() Chat$stream() Chat$stream_async() Chat$register_tool() Chat$clone()","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$new(provider, turns, seed = NULL, echo = \"none\")"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"provider provider object. turns unnamed list turns start chat (.e., continuing previous conversation). NULL zero-length list, conversation begins scratch. seed Optional integer seed ChatGPT uses try make output reproducible. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-get-turns-","dir":"Reference","previous_headings":"","what":"Method get_turns()","title":"A chat — Chat","text":"Retrieve turns sent received far (optionally starting system prompt, ).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_turns(include_system_prompt = FALSE)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"include_system_prompt Whether include system prompt turns (exists).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-set-turns-","dir":"Reference","previous_headings":"","what":"Method set_turns()","title":"A chat — Chat","text":"Replace existing turns new list.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$set_turns(value)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"value list Turns.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-get-system-prompt-","dir":"Reference","previous_headings":"","what":"Method get_system_prompt()","title":"A chat — Chat","text":"set, system prompt, , NULL.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_system_prompt()"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-set-system-prompt-","dir":"Reference","previous_headings":"","what":"Method set_system_prompt()","title":"A chat — Chat","text":"Update system prompt","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$set_system_prompt(value)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"value string giving new system prompt","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-tokens-","dir":"Reference","previous_headings":"","what":"Method tokens()","title":"A chat — Chat","text":"List number tokens consumed assistant turn. Currently tokens recorded assistant turns ; user turns zeros.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$tokens()"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-last-turn-","dir":"Reference","previous_headings":"","what":"Method last_turn()","title":"A chat — Chat","text":"last turn returned assistant.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$last_turn(role = c(\"assistant\", \"user\", \"system\"))"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"role Optionally, specify role find last turn role.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"A chat — Chat","text":"Either Turn NULL, turns specified role occurred.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"A chat — Chat","text":"Submit input chatbot, return response simple string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat(..., echo = NULL)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images (see content_image_file() content_image_url(). echo Whether emit response stdout received. NULL, value echo set chat object created used.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-extract-data-","dir":"Reference","previous_headings":"","what":"Method extract_data()","title":"A chat — Chat","text":"Extract structured data","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$extract_data(..., type, echo = \"none\", convert = TRUE)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. typically include phrase \"extract structured data\". type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers). convert Automatically convert JSON lists R data types using schema. example, turn arrays objects data frames arrays strings character vector.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-extract-data-async-","dir":"Reference","previous_headings":"","what":"Method extract_data_async()","title":"A chat — Chat","text":"Extract structured data, asynchronously. Returns promise resolves object matching type specification.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$extract_data_async(..., type, echo = \"none\")"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. typically include phrase \"extract structured data\". type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-chat-async-","dir":"Reference","previous_headings":"","what":"Method chat_async()","title":"A chat — Chat","text":"Submit input chatbot, receive promise resolves response . Returns promise resolves string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat_async(...)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-stream-","dir":"Reference","previous_headings":"","what":"Method stream()","title":"A chat — Chat","text":"Submit input chatbot, returning streaming results. Returns coro generator yields strings. iterating, generator block waiting content chatbot.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$stream(...)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-stream-async-","dir":"Reference","previous_headings":"","what":"Method stream_async()","title":"A chat — Chat","text":"Submit input chatbot, returning asynchronously streaming results. Returns coro async generator yields string promises.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$stream_async(...)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-register-tool-","dir":"Reference","previous_headings":"","what":"Method register_tool()","title":"A chat — Chat","text":"Register tool (R function) chatbot can use. chatbot decides use function,  ellmer automatically call submit results back. return value function. Generally, either string, JSON-serializable value. must direct control structure JSON returned, can return JSON-serializable value wrapped base::(), ellmer leave alone entire request JSON-serialized.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$register_tool(tool_def)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"tool_def Tool definition created tool().","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"A chat — Chat","text":"objects class cloneable method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$clone(deep = FALSE)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"deep Whether make deep clone.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A chat — Chat","text":"","code":"chat <- chat_openai(echo = TRUE) #> Using model = \"gpt-4o\". chat$chat(\"Tell me a funny joke\") #> Why don’t skeletons fight each other? #>  #> They don’t have the guts!"},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":null,"dir":"Reference","previous_headings":"","what":"Content types received from and sent to a chatbot — Content","title":"Content types received from and sent to a chatbot — Content","text":"Use functions writing package extends ellmer need customise methods various types content. normal use, see content_image_url() friends. ellmer abstracts away differences way different Providers represent various types content, allowing easily write code works chatbot. set classes represents types content can either sent received provider: ContentText: simple text (often markdown format). type content can streamed live received. ContentImageRemote ContentImageInline: images, either pointer remote URL included inline object. See content_image_file() friends convenient ways construct objects. ContentToolRequest: request perform tool call (sent assistant). ContentToolResult: result calling tool (sent user).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content()  ContentText(text = stop(\"Required\"))  ContentImage()  ContentImageRemote(url = stop(\"Required\"), detail = \"\")  ContentImageInline(type = stop(\"Required\"), data = NULL)  ContentToolRequest(   id = stop(\"Required\"),   name = stop(\"Required\"),   arguments = list() )  ContentToolResult(id = stop(\"Required\"), value = NULL, error = NULL)"},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Content types received from and sent to a chatbot — Content","text":"text single string. url URL remote image. detail currently used. type MIME type image. data Base64 encoded image data. id Tool call id (used associate request result) name Function name arguments Named list arguments call function . value, error Either results calling function succeeded, otherwise error message, string. One value error always NULL.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Content types received from and sent to a chatbot — Content","text":"S7 objects inherit Content","code":""},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content() #> <ellmer::Content> ContentText(\"Tell me a joke\") #> <ellmer::ContentText> #>  @ text: chr \"Tell me a joke\" ContentImageRemote(\"https://www.r-project.org/Rlogo.png\") #> <ellmer::ContentImageRemote> #>  @ url   : chr \"https://www.r-project.org/Rlogo.png\" #>  @ detail: chr \"\" ContentToolRequest(id = \"abc\", name = \"mean\", arguments = list(x = 1:5)) #> <ellmer::ContentToolRequest> #>  @ id       : chr \"abc\" #>  @ name     : chr \"mean\" #>  @ arguments:List of 1 #>  .. $ x: int [1:5] 1 2 3 4 5"},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":null,"dir":"Reference","previous_headings":"","what":"A chatbot provider — Provider","title":"A chatbot provider — Provider","text":"Provider captures details one chatbot service/API. captures API works, details underlying large language model. Different providers might offer (open source) model behind different API.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chatbot provider — Provider","text":"","code":"Provider(base_url = stop(\"Required\"), extra_args = list())"},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chatbot provider — Provider","text":"base_url base URL API. extra_args Arbitrary extra arguments included request body.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A chatbot provider — Provider","text":"S7 Provider object.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A chatbot provider — Provider","text":"add support new backend, need subclass Provider (adding additional fields provider needs) implement various generics control behavior provider.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A chatbot provider — Provider","text":"","code":"Provider(base_url = \"https://cool-models.com\") #> <ellmer::Provider> #>  @ base_url  : chr \"https://cool-models.com\" #>  @ extra_args: list()"},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":null,"dir":"Reference","previous_headings":"","what":"A user or assistant turn — Turn","title":"A user or assistant turn — Turn","text":"Every conversation chatbot consists pairs user assistant turns, corresponding HTTP request response. turns represented Turn object, contains list Contents representing individual messages within turn. might text, images, tool requests (assistant ), tool responses (user ). Note call $chat() related functions may result multiple user-assistant turn cycles. example, registered tools, ellmer automatically handle tool calling loop, may result number additional cycles. Learn tool calling vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A user or assistant turn — Turn","text":"","code":"Turn(role, contents = list(), json = list(), tokens = c(0, 0))"},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A user or assistant turn — Turn","text":"role Either \"user\", \"assistant\", \"system\". contents list Content objects. json serialized JSON corresponding underlying data turns. Currently provided assistant. useful information returned provider ellmer otherwise expose. tokens numeric vector length 2 representing number input output tokens (respectively) used turn. Currently recorded assistant turns.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A user or assistant turn — Turn","text":"S7 Turn object","code":""},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A user or assistant turn — Turn","text":"","code":"Turn(role = \"user\", contents = list(ContentText(\"Hello, world!\"))) #> <ellmer::Turn> #>  @ role    : chr \"user\" #>  @ contents:List of 1 #>  .. $ : <ellmer::ContentText> #>  ..  ..@ text: chr \"Hello, world!\" #>  @ json    : list() #>  @ tokens  : num [1:2] 0 0 #>  @ text    : chr \"Hello, world!\""},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":null,"dir":"Reference","previous_headings":"","what":"Type definitions for function calling and structured data extraction. — Type","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 classes provided use package devlopers extending ellmer. every day use, use type_boolean() friends.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(description = NULL, required = TRUE, type = stop(\"Required\"))  TypeEnum(description = NULL, required = TRUE, values = character(0))  TypeArray(description = NULL, required = TRUE, items = Type())  TypeObject(   description = NULL,   required = TRUE,   properties = list(),   additional_properties = TRUE )"},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type definitions for function calling and structured data extraction. — Type","text":"description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required component required? FALSE, component exist data, LLM may hallucinate value. applies element nested inside type_object(). type Basic type name. Must one boolean, integer, number, string. values Character vector permitted values. items type array items. Can created type_ function. properties Named list properties stored inside object. element S7 Type object.` additional_properties Can object arbitrary additional properties explicitly listed? supported Claude.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 objects inheriting Type","code":""},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(type = \"boolean\") #> <ellmer::TypeBasic> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ type       : chr \"boolean\" TypeArray(items = TypeBasic(type = \"boolean\")) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"boolean\""},{"path":"https://ellmer.tidyverse.org/reference/chat_azure.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Azure OpenAI — chat_azure","title":"Chat with a model hosted on Azure OpenAI — chat_azure","text":"Azure OpenAI server hosts number open source models well proprietary models OpenAI.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_azure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Azure OpenAI — chat_azure","text":"","code":"chat_azure(   endpoint = azure_endpoint(),   deployment_id,   api_version = NULL,   system_prompt = NULL,   turns = NULL,   api_key = azure_key(),   token = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_azure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Azure OpenAI — chat_azure","text":"endpoint Azure OpenAI endpoint url protocol hostname, .e. https://{-resource-name}.openai.azure.com. Defaults using value AZURE_OPENAI_ENDPOINT envinronment variable. deployment_id Deployment id model want use. api_version API version use. system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. api_key API key use authentication. generally supply directly, instead set AZURE_OPENAI_API_KEY environment variable. token Azure token authentication. typically required Azure OpenAI API calls, can used setup requires . api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_azure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Azure OpenAI — chat_azure","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_azure.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Azure OpenAI — chat_azure","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_azure(deployment_id = \"gpt-4o-mini\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_bedrock.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an AWS bedrock model — chat_bedrock","title":"Chat with an AWS bedrock model — chat_bedrock","text":"AWS Bedrock provides number chat based models, including Anthropic's Claude. Authenthication handled {paws.common}, authenthication work automatically, need follow advice https://www.paws-r-sdk.com/#credentials. particular, org uses AWS SSO, need run aws sso login terminal.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_bedrock.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an AWS bedrock model — chat_bedrock","text":"","code":"chat_bedrock(   system_prompt = NULL,   turns = NULL,   model = NULL,   profile = NULL,   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_bedrock.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an AWS bedrock model — chat_bedrock","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. profile AWS profile use. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_bedrock.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an AWS bedrock model — chat_bedrock","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_bedrock.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an AWS bedrock model — chat_bedrock","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_bedrock() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_claude.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an Anthropic Claude model — chat_claude","title":"Chat with an Anthropic Claude model — chat_claude","text":"Anthropic provides number chat based models Claude moniker. Note Claude Pro membership give ability call models via API; instead, need sign (pay ) developer account authenticate, recommend saving API key ANTHROPIC_API_KEY env var .Renviron (can easily edit calling usethis::edit_r_environ()).","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_claude.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an Anthropic Claude model — chat_claude","text":"","code":"chat_claude(   system_prompt = NULL,   turns = NULL,   max_tokens = 4096,   model = NULL,   api_args = list(),   base_url = \"https://api.anthropic.com/v1\",   api_key = anthropic_key(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_claude.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an Anthropic Claude model — chat_claude","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. max_tokens Maximum number tokens generate stopping. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. api_args Named list arbitrary extra arguments appended body every chat API call. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set ANTHROPIC_API_KEY environment variable. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_claude.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an Anthropic Claude model — chat_claude","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_claude.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an Anthropic Claude model — chat_claude","text":"","code":"chat <- chat_claude() chat$chat(\"Tell me three jokes about statisticians\") #> Here are three statistics-themed jokes: #>  #> 1. How do you tell the difference between an introverted statistician and #> an extroverted statistician? #> The extroverted statistician looks at *your* shoes when they're talking  #> to you. #>  #> 2. A statistician is someone who can put their head in an oven and their  #> feet in a freezer and say \"on average, I feel fine.\" #>  #> 3. Three statisticians go hunting. They spot a deer, and the first  #> statistician shoots, missing three feet to the left. The second  #> statistician shoots and misses three feet to the right. The third  #> statistician jumps up and shouts \"We got it! We got it! On average, we  #> hit it!\""},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"Chat LLM-powered Snowflake Cortex Analyst. Unlike comparable model APIs, Cortex take system prompt. Instead, caller must provide \"semantic model\" describing available tables, meaning, verified queries can run starting point. semantic model can passed YAML string via reference existing file Snowflake Stage. Note Cortex support multi-turn, remember previous messages. support registering tools, attempting result error.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"chat_cortex() picks following ambient Snowflake credentials: static OAuth token defined via SNOWFLAKE_TOKEN environment variable. Key-pair authentication credentials defined via SNOWFLAKE_USER SNOWFLAKE_PRIVATE_KEY (can PEM-encoded private key path one) environment variables. Posit Workbench-managed Snowflake credentials corresponding account.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"","code":"chat_cortex(   account = Sys.getenv(\"SNOWFLAKE_ACCOUNT\"),   credentials = NULL,   model_spec = NULL,   model_file = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"account Snowflake account identifier, e.g. \"testorg-test_account\". credentials list authentication headers pass httr2::req_headers(), function returns passed account parameter, NULL use ambient credentials. model_spec semantic model specification, NULL using model_file instead. model_file Path semantic model file stored Snowflake Stage, NULL using model_spec instead. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex","text":"","code":"if (FALSE) { # ellmer:::cortex_credentials_exist() chat <- chat_cortex(   model_file = \"@my_db.my_schema.my_stage/model.yaml\" ) chat$chat(\"What questions can I ask?\") }"},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Databricks — chat_databricks","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Databricks provides ---box access number foundation models can also serve gateway external models hosted third party. Databricks models support images, support structured outputs. Tool calling support also limited present; limited ellmer's tool calling features work properly .","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Databricks — chat_databricks","text":"chat_databricks() picks ambient Databricks credentials subset Databricks client unified authentication model. Specifically, supports: Personal access tokens Service principals via OAuth (OAuth M2M) User account via OAuth (OAuth U2M) Authentication via Databricks CLI Posit Workbench-managed credentials","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"chat_databricks(   workspace = databricks_workspace(),   system_prompt = NULL,   turns = NULL,   model = NULL,   token = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Databricks — chat_databricks","text":"workspace URL Databricks workspace, e.g. \"https://example.cloud.databricks.com\". use value environment variable DATABRICKS_HOST, set. system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. Available foundational models include: databricks-dbrx-instruct (default) databricks-mixtral-8x7b-instruct databricks-meta-llama-3-1-70b-instruct databricks-meta-llama-3-1-405b-instruct token authentication token Databricks workspace, NULL use ambient credentials. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_databricks() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_gemini.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a Google Gemini model — chat_gemini","title":"Chat with a Google Gemini model — chat_gemini","text":"authenticate, recommend saving API key GOOGLE_API_KEY env var .Renviron (can easily edit calling usethis::edit_r_environ()).","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_gemini.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a Google Gemini model — chat_gemini","text":"","code":"chat_gemini(   system_prompt = NULL,   turns = NULL,   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = gemini_key(),   model = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_gemini.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a Google Gemini model — chat_gemini","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GOOGLE_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_gemini.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a Google Gemini model — chat_gemini","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_gemini.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a Google Gemini model — chat_gemini","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_gemini() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on the GitHub model marketplace — chat_github","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"GitHub (via Azure) hosts number open source OpenAI models. access GitHub model marketplace, need apply accepted beta access program. See https://github.com/marketplace/models details. function lightweight wrapper around chat_openai() defaults tweaked GitHub model marketplace.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"chat_github(   system_prompt = NULL,   turns = NULL,   base_url = \"https://models.inference.ai.azure.com/\",   api_key = github_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead manage GitHub credentials described https://usethis.r-lib.org/articles/git-credentials.html. headless environments, also look GITHUB_PAT env var. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_github() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Groq — chat_groq","title":"Chat with a model hosted on Groq — chat_groq","text":"Sign https://groq.com. function lightweight wrapper around chat_openai() defaults tweaked groq. currently support structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"chat_groq(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.groq.com/openai/v1\",   api_key = groq_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Groq — chat_groq","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Groq — chat_groq","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_groq() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a local Ollama model — chat_ollama","title":"Chat with a local Ollama model — chat_ollama","text":"use chat_ollama() first download install Ollama. install models command line, e.g. ollama pull llama3.1 ollama pull gemma2. function lightweight wrapper around chat_openai() defaults tweaked ollama.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a local Ollama model — chat_ollama","text":"Tool calling supported streaming (.e. echo \"text\" \"\") Tool calling generally seems quite weak, least models tried .","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"chat_ollama(   system_prompt = NULL,   turns = NULL,   base_url = \"http://localhost:11434\",   model,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a local Ollama model — chat_ollama","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a local Ollama model — chat_ollama","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_ollama(model = \"llama3.2\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an OpenAI model — chat_openai","title":"Chat with an OpenAI model — chat_openai","text":"OpenAI provides number chat-based models, mostly ChatGPT brand. Note ChatGPT Plus membership grant access API. need sign developer account (pay ) developer platform. authentication, recommend saving API key OPENAI_API_KEY environment variable .Renviron file. can easily edit file calling usethis::edit_r_environ().","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat_openai(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.openai.com/v1\",   api_key = openai_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an OpenAI model — chat_openai","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an OpenAI model — chat_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4o\". chat$chat(\"   What is the difference between a tibble and a data frame?   Answer with a bulleted list \") #> - **Class and Structure**: #>   - A tibble is a modern version of the data frame, introduced in the  #> `tibble` package, which is part of the tidyverse collection in R. #>   - A data frame is the base R structure for storing tabular data. #>  #> - **Printing Behavior**: #>   - Tibbles have a refined print method displaying only the first 10 rows #> and the columns that fit on screen, providing a cleaner output. #>   - Data frames print the whole dataset unless specifically instructed  #> otherwise, which can be unwieldy for large datasets. #>  #> - **Column Types**: #>   - Tibbles do not change the types of the input data: they respect  #> column types and do not automatically convert strings to factors. #>   - Data frames may automatically convert character vectors to factors  #> unless stringsAsFactors is set to FALSE. #>  #> - **Subsetting**: #>   - Subsetting a tibble with a single bracket (`[...]`) always returns a  #> tibble, preserving the data frame structure. #>   - With data frames, subsetting can return a vector if selecting a  #> single column without the `drop = FALSE` argument. #>  #> - **Name Validation**: #>   - Tibbles do not automatically rename or change invalid column names  #> (e.g., names with spaces). They allow non-standard names by using  #> backticks. #>   - Data frames typically force names to adhere to valid R variable  #> names, converting them as necessary. #>  #> - **Use within Tidyverse**: #>   - Tibbles are integral to the tidyverse and are designed to work  #> seamlessly with other tidy tools for data manipulation and visualization. #>   - Data frames are the default R structure and are not as tightly  #> integrated into the tidyverse toolkit. #>  #> - **Error and Warning Messages**: #>   - Tibbles provide more user-friendly error and warning messages  #> compared to the more generic messages returned by operations on data  #> frames. #>  #> These differences make tibbles particularly useful when working within  #> the tidyverse ecosystem, offering more intuitive and robust data handling #> for modern data science workflows.  chat$chat(\"Tell me three funny jokes about statistcians\") #> Sure, here are three jokes about statisticians that might give you a  #> chuckle: #>  #> 1. Why don’t statisticians play hide and seek? #>    - Because good luck hiding from someone who always checks the outliers #> first! #>  #> 2. How many statisticians does it take to change a light bulb? #>    - Just one, but they’ll need to test 20 different light bulbs to  #> determine the mean change time! #>  #> 3. Two statisticians walk into a bar.  #>    - The third one ducks. #>  #> I hope these bring a smile to your face!"},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on perplexity.ai — chat_perplexity","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Sign https://www.perplexity.ai. Perplexity AI platform running LLMs capable searching web real-time help answer questions information may available model trained. function lightweight wrapper around chat_openai() defaults tweaked Perplexity AI.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"chat_perplexity(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.perplexity.ai/\",   api_key = perplexity_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set PERPLEXITY_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_perplexity() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted by vLLM — chat_vllm","title":"Chat with a model hosted by vLLM — chat_vllm","text":"vLLM open source library provides efficient convenient LLMs model server. can use chat_vllm() connect endpoints powered vLLM.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"chat_vllm(   base_url,   system_prompt = NULL,   turns = NULL,   model,   seed = NULL,   api_args = list(),   api_key = vllm_key(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted by vLLM — chat_vllm","text":"base_url base URL endpoint; default uses OpenAI. system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. api_key API key use authentication. generally supply directly, instead set VLLM_API_KEY environment variable. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted by vLLM — chat_vllm","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_vllm(\"http://my-vllm.com\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode image content for chat input — content_image_url","title":"Encode image content for chat input — content_image_url","text":"functions used prepare image URLs files input chatbot. content_image_url() function used provide URL image, content_image_file() used provide image data .","code":""},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode image content for chat input — content_image_url","text":"","code":"content_image_url(url, detail = c(\"auto\", \"low\", \"high\"))  content_image_file(path, content_type = \"auto\", resize = \"low\")  content_image_plot(width = 768, height = 768)"},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode image content for chat input — content_image_url","text":"url URL image include chat input. Can data: URL regular URL. Valid image types PNG, JPEG, WebP, non-animated GIF. detail detail setting image. Can \"auto\", \"low\", \"high\". path path image file include chat input. Valid file extensions .png, .jpeg, .jpg, .webp, (non-animated) .gif. content_type content type image (e.g. image/png). \"auto\", content type inferred file extension. resize \"low\", resize images fit within 512x512. \"high\", resize fit within 2000x768 768x2000. (See OpenAI docs specific sizes used.) \"none\", resize. can also pass custom string resize image specific size, e.g. \"200x200\" resize 200x200 pixels preserving aspect ratio. Append > resize image larger specified size, ! ignore aspect ratio (e.g. \"300x200>!\"). values none require magick package. width, height Width height pixels.","code":""},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode image content for chat input — content_image_url","text":"input object suitable including ... parameter chat(), stream(), chat_async(), stream_async() methods.","code":""},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encode image content for chat input — content_image_url","text":"","code":"chat <- chat_openai(echo = TRUE) #> Using model = \"gpt-4o\". chat$chat(   \"What do you see in these images?\",   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   content_image_file(system.file(\"httr2.png\", package = \"ellmer\")) ) #> The first image is the logo of the R programming language, which features #> a large blue \"R\" over a gray stylized \"C.\" #>  #> The second image is a hexagonal logo for the \"httr2\" package in R. It  #> shows the text \"httr2\" with a silhouette of a baseball player swinging a  #> bat, set against a dark blue background.  plot(waiting ~ eruptions, data = faithful)  chat <- chat_openai(echo = TRUE) #> Using model = \"gpt-4o\". chat$chat(   \"Describe this plot in one paragraph, as suitable for inclusion in    alt-text. You should briefly describe the plot type, the axes, and    2-5 major visual patterns.\",    content_image_plot() ) #> This image is a line plot displaying the exponential growth of a  #> function. The x-axis represents time while the y-axis shows the  #> corresponding values of the function. A notable pattern is the steep  #> incline starting at the origin and rapidly increasing as it moves to the  #> right, demonstrating the classic J-shaped curve characteristic of  #> exponential growth functions. The line consistently rises without any  #> dips or plateaus, emphasizing continuous and accelerating growth."},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Format contents into a textual representation — contents_text","title":"Format contents into a textual representation — contents_text","text":"generic functions can use convert Turn contents Content objects textual representations. contents_text() minimal includes ContentText objects output. contents_markdown() returns text content (assumes markdown convert ) plus markdown representations images content types. contents_html() returns text content, converted markdown HTML commonmark::markdown_html(), plus HTML representations images content types.","code":""},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format contents into a textual representation — contents_text","text":"","code":"contents_text(content, ...)  contents_html(content, ...)  contents_markdown(content, ...)"},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format contents into a textual representation — contents_text","text":"content Turn Content object converted text. contents_markdown() also accepts Chat instances turn entire conversation history markdown text. ... Additional arguments passed methods.","code":""},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format contents into a textual representation — contents_text","text":"string text, markdown HTML.","code":""},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format contents into a textual representation — contents_text","text":"","code":"turns <- list(   Turn(\"user\", contents = list(     ContentText(\"What's this image?\"),     content_image_url(\"https://placehold.co/200x200\")   )),   Turn(\"assistant\", \"It's a placeholder image.\") )  lapply(turns, contents_text) #> [[1]] #> [1] \"What's this image?\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  lapply(turns, contents_markdown) #> [[1]] #> [1] \"What's this image?\\n\\n![](https://placehold.co/200x200)\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  if (rlang::is_installed(\"commonmark\")) {   contents_html(turns[[1]]) } #> [1] \"<p>What's this image?<\/p>\\n\\n<img src=\\\"https://placehold.co/200x200\\\">\""},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Create metadata for a tool — create_tool_def","title":"Create metadata for a tool — create_tool_def","text":"order use function tool chat, need craft right call tool(). function helps documented functions extracting function's R documentation creating tool() call , using LLM. meant used interactively writing code, part final code. function package documentation, used. Otherwise, source code function can automatically detected, comments immediately preceding function used (especially helpful Roxygen comments). neither available, just function signature used. Note function inherently imperfect. handle possible R functions, parameters suitable use tool call (example, serializable simple JSON objects). documentation might specify expected shape arguments level detail allow exact JSON schema generated. Please sure review generated code using !","code":""},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create metadata for a tool — create_tool_def","text":"","code":"create_tool_def(topic, model = \"gpt-4o\", echo = interactive(), verbose = FALSE)"},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create metadata for a tool — create_tool_def","text":"topic symbol string literal naming function create metadata . Can also expression form pkg::fun. model OpenAI model use generating metadata. Defaults \"gpt-4o\". echo Emit registration code console. Defaults TRUE interactive sessions. verbose TRUE, print input send LLM, may useful debugging unexpectedly poor results.","code":""},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create metadata for a tool — create_tool_def","text":"register_tool call can copy paste code. Returned invisibly echo TRUE.","code":""},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create metadata for a tool — create_tool_def","text":"","code":"if (FALSE) { # \\dontrun{   # These are all equivalent   create_tool_def(rnorm)   create_tool_def(stats::rnorm)   create_tool_def(\"rnorm\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/ellmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"ellmer: Call Large Language Model 'API's — ellmer-package","title":"ellmer: Call Large Language Model 'API's — ellmer-package","text":"consistent interface calling 'LLM' (large language model) APIs works across different providers including 'Claude', 'OpenAI' 'Azure' 'Bedrock' 'Google' 'Gemini'. Supports streaming, 'async', tool calling, structured data extraction.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/ellmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"ellmer: Call Large Language Model 'API's — ellmer-package","text":"Maintainer: Hadley Wickham hadley@posit.co Authors: Joe Cheng contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":null,"dir":"Reference","previous_headings":"","what":"Helpers for interpolating data into prompts — interpolate","title":"Helpers for interpolating data into prompts — interpolate","text":"functions lightweight wrappers around glue make easier interpolate dynamic data static prompt. Compared glue, functions expect wrap dynamic values {{ }}, making easier include R code JSON prompt.","code":""},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"interpolate(prompt, ..., .envir = parent.frame())  interpolate_file(path, ..., .envir = parent.frame())"},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helpers for interpolating data into prompts — interpolate","text":"prompt prompt string. generally expose end user, since glue interpolation makes easy run arbitrary code. ... Define additional temporary variables substitution. .envir Environment evaluate ... expressions . Used wrapping another function. See vignette(\"wrappers\", package = \"glue\") details. path path prompt file (often .md).","code":""},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helpers for interpolating data into prompts — interpolate","text":"{glue} string.","code":""},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"joke <- \"You're a cool dude who loves to make jokes. Tell me a joke about {{topic}}.\"  # You can supply valuese directly: interpolate(joke, topic = \"bananas\") #> You're a cool dude who loves to make jokes. Tell me a joke about bananas.  # Or allow interpolate to find them in the current environment: topic <- \"applies\" interpolate(joke) #> You're a cool dude who loves to make jokes. Tell me a joke about applies."},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a live chat application — live_console","title":"Open a live chat application — live_console","text":"live_console() lets chat interactively console. live_browser() lets chat interactively browser. Note functions mutate input chat object chat turns appended history.","code":""},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a live chat application — live_console","text":"","code":"live_console(chat, quiet = FALSE)  live_browser(chat, quiet = FALSE)"},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a live chat application — live_console","text":"chat chat object created chat_openai() friends. quiet TRUE, suppresses initial message explains use console.","code":""},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open a live chat application — live_console","text":"(Invisibly) input chat.","code":""},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open a live chat application — live_console","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_claude() live_console(chat) live_browser(chat) } # }"},{"path":"https://ellmer.tidyverse.org/reference/token_usage.html","id":null,"dir":"Reference","previous_headings":"","what":"Report on token usage in the current session — token_usage","title":"Report on token usage in the current session — token_usage","text":"Call function find cumulative number tokens sent recieved current session.","code":""},{"path":"https://ellmer.tidyverse.org/reference/token_usage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage()"},{"path":"https://ellmer.tidyverse.org/reference/token_usage.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Report on token usage in the current session — token_usage","text":"data frame","code":""},{"path":"https://ellmer.tidyverse.org/reference/token_usage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage() #>                               name input output #> 1 OpenAI-https://api.openai.com/v1  2063    693 #> 2                           Claude    14    157"},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a tool — tool","title":"Define a tool — tool","text":"Define R function use chatbot. function always run current R instance. Learn vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a tool — tool","text":"","code":"tool(.fun, .description, ..., .name = NULL)"},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a tool — tool","text":".fun function invoked tool called. .description detailed description function . Generally, information can provide , better. ... Name-type pairs define arguments accepted function. element created type_*() function. .name name function.","code":""},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a tool — tool","text":"S7 ToolDef object.","code":""},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a tool — tool","text":"","code":"# First define the metadata that the model uses to figure out when to # call the tool tool_rnorm <- tool(   rnorm,   \"Drawn numbers from a random normal distribution\",   n = type_integer(\"The number of observations. Must be a positive integer.\"),   mean = type_number(\"The mean value of the distribution.\"),   sd = type_number(\"The standard deviation of the distribution. Must be a non-negative number.\") ) chat <- chat_openai() #> Using model = \"gpt-4o\". # Then register it chat$register_tool(tool_rnorm)  # Then ask a question that needs it. chat$chat(\"   Give me five numbers from a random normal distribution. \") #> To provide you with five numbers from a random normal distribution, I  #> need the mean and standard deviation values for the distribution. Could  #> you please specify these parameters?  # Look at the chat history to see how tool calling works: # Assistant sends a tool request which is evaluated locally and # results are send back in a tool result."},{"path":"https://ellmer.tidyverse.org/reference/type_boolean.html","id":null,"dir":"Reference","previous_headings":"","what":"Type specifications — type_boolean","title":"Type specifications — type_boolean","text":"functions specify object types way chatbots understand used tool calling structured data extraction. names based JSON schema, APIs expect behind scenes. translation R concepts types fairly straightforward. type_boolean(), type_integer(), type_number(), type_string() represent scalars. equivalent length-1 logical, integer, double, character vectors (respectively). type_enum() equivalent length-1 factor; string can take specified values. type_array() equivalent vector R. can use represent atomic vector: e.g. type_array(items = type_boolean()) equivalent logical vector type_array(items = type_string()) equivalent character vector). can also use represent list complicated types every element type (R base equivalent ), e.g. type_array(items = type_array(items = type_string())) represents list character vectors. type_object() equivalent named list R, every element must specified type. example, type_object(= type_string(), b = type_array(type_integer())) equivalent list element called string element called b integer vector.","code":""},{"path":"https://ellmer.tidyverse.org/reference/type_boolean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type specifications — type_boolean","text":"","code":"type_boolean(description = NULL, required = TRUE)  type_integer(description = NULL, required = TRUE)  type_number(description = NULL, required = TRUE)  type_string(description = NULL, required = TRUE)  type_enum(description = NULL, values, required = TRUE)  type_array(description = NULL, items, required = TRUE)  type_object(   .description = NULL,   ...,   .required = TRUE,   .additional_properties = FALSE )"},{"path":"https://ellmer.tidyverse.org/reference/type_boolean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type specifications — type_boolean","text":"description, .description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required, .required component required? FALSE, component exist data, LLM may hallucinate value. applies element nested inside type_object(). values Character vector permitted values. items type array items. Can created type_ function. ... Name-type pairs defineing components object must possess. .additional_properties Can object arbitrary additional properties explicitly listed? supported Claude.","code":""},{"path":"https://ellmer.tidyverse.org/reference/type_boolean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type specifications — type_boolean","text":"","code":"# An integer vector type_array(items = type_integer()) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"integer\"  # The closest equivalent to a data frame is an array of objects type_array(items = type_object(    x = type_boolean(),    y = type_string(),    z = type_number() )) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeObject> #>  .. @ description          : NULL #>  .. @ required             : logi TRUE #>  .. @ properties           :List of 3 #>  .. .. $ x: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"boolean\" #>  .. .. $ y: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"string\" #>  .. .. $ z: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"number\" #>  .. @ additional_properties: logi FALSE  # There's no specific type for dates, but you use a string with the # requested format in the description (it's not gauranteed that you'll # get this format back, but you should most of the time) type_string(\"The creation date, in YYYY-MM-DD format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The creation date, in YYYY-MM-DD format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\" type_string(\"The update date, in dd/mm/yyyy format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The update date, in dd/mm/yyyy format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\""},{"path":"https://ellmer.tidyverse.org/news/index.html","id":"ellmer-development-version","dir":"Changelog","previous_headings":"","what":"ellmer (development version)","title":"ellmer (development version)","text":"New chat_vllm() chat models served vLLM (#140). default chat_openai() model now GPT-4o. New Chat$set_turns() set turns. Chat$turns() now Chat$get_turns(). Chat$system_prompt() replaced Chat$set_system_prompt() Chat$get_system_prompt(). Async streaming async chat now event-driven use later::later_fd() wait efficiently curl socket activity (#157). New chat_bedrock() chat AWS bedrock models (#50). New chat$extract_data() uses structured data API available (tool calling otherwise) extract data structured according known type specification. can create specs functions type_boolean(), type_integer(), type_number(), type_string(), type_enum(), type_array(), type_object() (#31). general ToolArg() replaced specific type_*() functions. ToolDef() renamed tool. content_image_url() now create inline images given data url (#110). Streaming ollama results works (#117). Streaming OpenAI results now capture results, including logprops (#115). New interpolate() prompt_file() make easier create prompts mix static text dynamic values. can find many tokens ’ve used current session calling token_usage(). chat_browser() chat_console() now live_browser() live_console(). echo can now one three values: “none”, “text”, “”. “”, ’ll now see user assistant turns, content types printed, just text. running global environment, echo defaults “text”, running inside function defaults “none”. can now log low-level JSON request/response info setting options(ellmer_verbosity = 2). chat$register_tool() now takes object created Tool(). makes little easier reuse tool definitions (#32). new_chat_openai() now chat_openai(). Claude Gemini now supported via chat_claude() chat_gemini(). Snowflake Cortex Analyst now supported via chat_cortex() (#56). Databricks now supported via chat_databricks() (#152).","code":""}]
