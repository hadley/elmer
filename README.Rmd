---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# elmer <a href="https://hadley.github.io/elmer/"><img src="man/figures/logo.png" align="right" height="138" alt="elmer website" /></a>

<!-- badges: start -->
[![R-CMD-check](https://github.com/hadley/elmer/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/hadley/elmer/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

The goal of elmer is to provide a user friendly wrapper over the most common APIs for calling llm's. Major design goals include support for streaming and making it easy to register and call R functions.

## Installation

You can install the development version of elmer from [GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("hadley/elmer")
```

## Prerequisites

To use elmer, you need an OpenAI API key. You can get one from [your developer console](https://platform.openai.com/account/api-keys). Then you should save that value as the `OPENAI_API_KEY` environment variable in your `~/.Renviron` (an easy way to open that file is to call `usethis::use_renviron()`).

## Using elmer

You chat with elmer in several different ways, depending on whether you are working interactively or programmatically. They all start with creating a new chat object:

```r
library(elmer)

chat <- new_chat_openai(
  model = "gpt-4o-mini",
  system_prompt = "You are a friendly but terse assistant.",
  echo = TRUE
)
```

Chat objects are stateful: they retain the context of the conversation, so each new query can build on the previous ones. This is true regardless of which of the various ways of chatting you use.

### Interactive chat console

The most interactive, least programmatic way of using elmer is to chat with it directly in your R console.

```{r eval=FALSE}
chat$console()
```
```
╔════════════════════════════════════════════════════════╗
║  Entering chat console. Use """ for multi-line input.  ║
║  Press Ctrl+C to quit.                                 ║
╚════════════════════════════════════════════════════════╝
>>> Who were the original creators of R?
R was originally created by Ross Ihaka and Robert Gentleman at the University of
Auckland, New Zealand.

>>> When was that?
R was initially released in 1995. Development began a few years prior to that,
in the early 1990s.
```

The chat console is useful for quickly exploring the capabilities of the model, especially when you've customized the chat object with tool integrations (see below).

Again, keep in mind that the chat object retains state, so when you enter the chat console, any previous interactions with that chat object are still part of the conversation, and any interactions you have in the chat console will persist even after you exit back to the R prompt.

### Interactive method call

The second most interactive way to chat using elmer is to call the `chat()` method.

```{r eval=FALSE}
chat$chat("What preceding languages most influenced R?")
```
```
R was primarily influenced by the S programming language, particularly S-PLUS.
Other languages that had an impact include Scheme and various data analysis
languages.
```

If you initialize the chat object with `echo = TRUE`, as we did above, the `chat` method streams the response to the console as it arrives. When the entire response is received, it is returned as a character vector (invisibly, so it's not printed twice).

This mode is useful when you want to see the response as it arrives, but you don't want to enter the chat console.

#### Vision (image input)

If you want to ask a question about an image, you can pass one or more additional input arguments using `content_image_file()` and/or `content_image_url()`.

```{r eval=FALSE}
chat$chat(
  content_image_url("https://www.r-project.org/Rlogo.png"),
  "Can you explain this logo?"
)
```
```
The logo of R features a stylized letter "R" in blue, enclosed in an oval shape that resembles the letter "O,"
signifying the programming language's name. The design conveys a modern and professional look, reflecting its use
in statistical computing and data analysis. The blue color often represents trust and reliability, which aligns
with R's role in data science.
```

The `content_image_url` function takes a URL to an image file and sends that URL directly to the API. The `content_image_file` function takes a path to a local image file and encodes it as a base64 string to send to the API. Note that by default, `content_image_file` automatically resizes the image to fit within 512x512 pixels; set the `resize` parameter to `"high"` if higher resolution is needed.

### Programmatic chat

If you don't want to see the response as it arrives, you can turn off echoing by leaving off the `echo = TRUE` argument to `new_chat_openai()`.

```{r eval=FALSE}
chat <- new_chat_openai(
  model = "gpt-4o-mini",
  system_prompt = "You are a friendly but terse assistant."
)
chat$chat("Is R a functional programming language?")
```
```
[1] "Yes, R supports functional programming concepts. It allows functions to be first-class objects, supports higher-order functions, and encourages the use of functions as core components of code. However, it also supports procedural and object-oriented programming styles."
```

This mode is useful for programming using elmer, when the result is either not intended for human consumption or when you want to process the response before displaying it.

### Streaming results

The `chat()` method does not return any results until the entire response has been received. (It can _print_ the streaming results to the console, but it _returns_ the result only when the response is complete.)

If you want to process the response as it arrives, you can use the `stream()` method. This may be useful when you want to display the response in realtime, but somewhere other than the R console (like writing to a file, or an HTTP response, or a Shiny chat window); or when you want to manipulate the response before displaying it, without giving up the immediacy of streaming.

The `stream()` method returns a [generator](https://coro.r-lib.org/articles/generator.html) from the [coro package](https://coro.r-lib.org/), which you can loop over to process the response as it arrives.

```{r eval=FALSE}
stream <- chat$stream("What are some common uses of R?")
coro::loop(for (chunk in stream) {
  cat(toupper(chunk))
})
```
```
R IS COMMONLY USED FOR:

1. **STATISTICAL ANALYSIS**: PERFORMING COMPLEX STATISTICAL TESTS AND ANALYSES.
2. **DATA VISUALIZATION**: CREATING GRAPHS, CHARTS, AND PLOTS USING PACKAGES LIKE GGPLOT2.
3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR AND TIDYR.
4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS WITH LIBRARIES LIKE CARET AND RANDOMFOREST.
5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA AND GENOMIC STUDIES.
6. **ECONOMETRICS**: PERFORMING ECONOMIC DATA ANALYSIS AND MODELING.
7. **REPORTING**: GENERATING DYNAMIC REPORTS AND DASHBOARDS WITH R MARKDOWN.
8. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA AND FORECASTING.

THESE USES MAKE R A POWERFUL TOOL FOR DATA SCIENTISTS, STATISTICIANS, AND RESEARCHERS.
```

## Async usage (Advanced)

elmer also supports async usage, which is useful when you want to run multiple chat sessions concurrently. This is primarily useful in Shiny applications, where using the methods described above would block the Shiny app for other users for the duration of each response.

To use async chat, instead of `chat()`/`stream()`, call `chat_async()`/`stream_async()`. The `_async` variants take the same arguments for construction, but return promises instead of the actual response.

Remember that chat objects are stateful, maintaining the conversation history as you interact with it. Note that this means it doesn't make sense to issue multiple chat/stream operations on the same chat object concurrently, as the conversation history could become corrupted with interleaved conversation fragments. If you need to run multiple chat sessions concurrently, create multiple chat objects.

### Asynchronous chat

For asynchronous, non-streaming chat, you use the `chat()` method as before, but handle the result as a promise instead of a string.

```{r eval=FALSE}
library(promises)

chat$chat_async("How's your day going?") %...>% print()
```
```
I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions you have.
```

TODO: Shiny example

### Asynchronous streaming

For asynchronous streaming, you use the `stream()` method as before, but the result is a [async generator](https://coro.r-lib.org/reference/async_generator.html) from the [coro package](https://coro.r-lib.org/). This is the same as a regular [generator](https://coro.r-lib.org/articles/generator.html), except instead of giving you strings, it gives you promises that resolve to strings.

```{r eval=FALSE}
stream <- chat$stream_async("What are some common uses of R?")
coro::async(function() {
  for (chunk in await_each(stream)) {
    cat(toupper(chunk))
  }
})()
```
```
<Promise [pending]>
>
R IS COMMONLY USED FOR:

1. **STATISTICAL ANALYSIS**: PERFORMING VARIOUS STATISTICAL TESTS AND MODELS.
2. **DATA VISUALIZATION**: CREATING PLOTS AND GRAPHS TO VISUALIZE DATA.
3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR.
4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS AND ALGORITHMS.
5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA, ESPECIALLY IN GENOMICS.
6. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA FOR TRENDS AND FORECASTS.
7. **REPORT GENERATION**: CREATING DYNAMIC REPORTS WITH R MARKDOWN.
8. **GEOSPATIAL ANALYSIS**: MAPPING AND ANALYZING GEOGRAPHIC DATA.
```

Async generators are very advanced, and require a good understanding of asynchronous programming in R. They are also the only way to present streaming results in Shiny without blocking other users. Fortunately, Shiny will soon have chat components that will make this easier, where you can simply hand the result of `stream_async()` to a chat output.

## Tool calling (a.k.a. function calling)

One of the most interesting aspects of modern chat models is their ability to make use of external tools that are defined by the caller.

When making a chat request to the chat model, the caller advertises one or more tools (defined by their function name, description, and a list of expected arguments), and the chat model can choose to respond with one or more "tool calls". These tool calls are requests _from the chat model to the caller_ to execute the function with the given arguments; the caller is expected to execute the functions and "return" the results by submitting another chat request with the conversation so far, plus the results. The chat model can then use those results in formulating its response, or, it may decide to make additional tool calls.

*Note that the chat model does not directly execute any external tools!* It only makes requests for the caller to execute them. The value that the chat model brings is not in helping with execution, but with knowing when it makes sense to call a tool, what values to pass as arguments, and how to use the results in formulating its response.

### Motivating example

Let's take a look at an example where we really need an external tool. Chat models generally do not know the current time, which makes questions like these impossible.

```{r eval=FALSE}
chat <- new_chat_openai(model = "gpt-4o")
chat$chat("How long ago exactly was the moment Neil Armstrong touched down on the moon?")
```
```
Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. To determine how long ago that
was from the current year of 2023, we can calculate the difference in years, months, and days.

From July 20, 1969, to July 20, 2023, is exactly 54 years. If today's date is after July 20, 2023, you
would add the additional time since then. If it is before, you would consider slightly less than 54
years.

As of right now, can you confirm the current date so we can calculate the precise duration?
```

Unfortunately, this example was run on September 18, 2024. Let's give the chat model the ability to determine the current time and try again.

### Defining a tool function

The first thing we'll do is define an R function that returns the current time. This will be our tool.

```{r eval=FALSE}
#' Gets the current time in the given time zone.
#'
#' @param tz The time zone to get the current time in.
#' @return The current time in the given time zone.
get_current_time <- function(tz = "UTC") {
  format(Sys.time(), tz = tz, usetz = TRUE)
}
```

Note that we've gone through the trouble of creating [roxygen2 comments](https://roxygen2.r-lib.org/). This is a very important step that will help the model use your tool correctly!

Let's test it:

```{r eval=FALSE}
get_current_time()
```
```
[1] "2024-09-18 17:47:14 UTC"
```

### Registering tools

Now we need to tell our chat object about our `get_current_time` function. This is done using the `register_tool()` method.

```{r eval=FALSE}
chat <- new_chat_openai(model = "gpt-4o")

chat$register_tool(
  fun = get_current_time,
  description = "Gets the current time in the given time zone.",
  arguments = list(
    tz = tool_arg(
      type = "string",
      description = "The time zone to get the current time in. Defaults to `\"UTC\"`.",
      required = FALSE
    )
  )
)
```

This is a fair amount of code to write, even for such a simple function as `get_current_time`. Fortunately, you don't have to write this by hand! I generated the above `register_tool` call by calling `create_tool_metadata(get_current_time)`, which printed that code at the console. `create_tool_metadata()` works by passing the function's signature and documentation to GPT-4o, and asking it to generate the `register_tool` call for you.

Note that `create_tool_metadata()` may not create perfect results, so you must review the generated code before using it. But it is a huge time-saver nonetheless, and removes the tedious boilerplate generation you'd have to do otherwise.

### Using the tool

That's all we need to do! Let's retry our query:

```{r eval=FALSE}
chat$chat("How long ago exactly was the moment Neil Armstrong touched down on the moon?")
```
```
Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC.

To calculate the time elapsed from that moment until the current time (September 18, 2024, 17:47:19
UTC), we need to break it down.

1. From July 20, 1969, 20:17 UTC to July 20, 2024, 20:17 UTC is exactly 55 years.
2. From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, we need to further break down:

   - From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, which is:
     - 1 full month (August)
     - 30 – 20 = 10 days of July
     - 18 days of September until 17:47:19 UTC

So, in detail:
   - 55 years
   - 1 month
   - 28 days
   - From July 20, 2024, 20:17 UTC to July 20, 2024, 17:47:19 UTC: 23 hours, 30 minutes, and 19 seconds

Time Total:
- 55 years
- 1 month
- 28 days
- 23 hours
- 30 minutes
- 19 seconds

This is the exact time that has elapsed since Neil Armstrong's historic touchdown on the moon.
```

That's correct! Without any further guidance, the chat model decided to call our tool function and successfully used its result in formulating its response.

(Full disclosure: I originally tried this example with the default model of `gpt-4o-mini` and it got the tool calling right but the date math wrong, hence the explicit `model="gpt-4o"`.)

This tool example was extremely simple, but you can imagine doing much more interesting things from tool functions: calling APIs, reading from or writing to a database, kicking off a complex simulation, or even calling a complementary GenAI model (like an image generator). Or if you are using elmer in a Shiny app, you could use tools to set reactive values, setting off a chain of reactive updates.

### Tool limitations

Remember that tool arguments come from the chat model, and tool results are returned to the chat model. That means that only simple, `{jsonlite}` compatible data types can be used as inputs and outputs. It's highly recommended that you stick to strings/character, numbers, booleans/logical, null, and named or unnamed lists of those types. And you can forget about using functions, environments, external pointers, R6 classes, and other complex R objects as arguments or return values. Returning data frames seems to work OK, although be careful not to return too much data, as it all counts as tokens (i.e., they count against your context window limit and also cost you money).
